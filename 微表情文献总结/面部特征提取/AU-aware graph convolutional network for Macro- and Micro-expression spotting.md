# AU-aware graph convolutional network for Macro- and Micro-expression spotting

基于AU感知的宏表情和微表情识别的图卷积网络  

DOI:10.48550/arXiv.2303.09114  

## 论文详细分析

### 摘要

> 建模面部感兴趣区域（ROI）之间的关系来提取更精细的空间特征

1.如何建模建立关系？  

2.什么是空间特征？  

参考:

[GCN—图卷积神经网络理解_gcn提取空间特征-CSDN博客](https://blog.csdn.net/wsp_1138886114/article/details/100709692)

[机器学习术语表 | Freeopen](https://freeopen.github.io/glossary/)

这里的空间特征是机器学习中的概念。

对于图片而言，中心点像素及其邻域像素构成空间，这些像素的值和关系构成特征。




> 提出了一个基于图卷积的网络，成为动作单元感知图卷积网络（AUW-GCN）

1.什么是图卷积？  

参考：

[GCN—图卷积神经网络理解_gcn提取空间特征-CSDN博客](https://blog.csdn.net/wsp_1138886114/article/details/100709692)

卷积一般指离散卷积，其本质是加权求和。

通过计算中心像素点以及相邻像素点的加权和来构成特征图谱以实现空间特征的提取。

传统的CNN（卷积神经网络）处理的图像或者视频的像素点是排列的很整齐的矩阵，但实际上有许多数据及其关系不是规整的矩阵，其网络结构为拓扑图，而GCN就是用于处理拓扑结构，用于弥补CNN无法处理拓扑图的缺陷。

图卷积是将一个节点周围的邻居按照不同的权重叠加起来，每个节点的周围的的邻居数不固定。但是一般卷积的节点的邻居数是固定的。

2.什么是基于图卷积的网络？  

参考：

[图神经网络之图卷积网络——GCN_图卷积的神经网络-CSDN博客](https://blog.csdn.net/zbp_12138/article/details/110246797)

有一些公式不太理解。

图卷积相比图像卷积更灵活。



> 为了注入先验信息和解决小数据集的问题，将AU相关的统计信息编码到网络中

1.什么是注入先验信息？先验信息是什么？

参考：

[如何将先验知识注入推荐模型-CSDN博客](https://blog.csdn.net/Kaiyuan_sjtu/article/details/121987636)

是不是预处理模型？

2.小数据集是？

参考：

[【干货指南】机器学习必须需要大量数据？小数据集也能有大价值！ (qq.com)](https://mp.weixin.qq.com/s/xGnDcRtKKt4FyVRAMPSqYA)

3.如何将AU的统计信息编码到网络中？  

参考：

[【论文阅读】AU检测|《Deep Adaptive Attention for Joint Facial Action Unit Detection and Face Alignment》_au 关键点检测-CSDN博客](https://blog.csdn.net/qq_43521527/article/details/113440970)

不太能理解。



> 在CAS(ME)2和SAMM-LV两个基准数据集上取得了新的SOTA性能

1.SOTA性能是什么?  

参考：

[论文里SOTA性能的理解-CSDN博客](https://blog.csdn.net/wuyeyoulan23/article/details/123756127)

大概是最好、最先进的意思。



### 引言

> 微表情研究的一个重要部分是微表情的定位（发现），包括在未剪辑的长视频中定位表情间隔

1.如何定位?



> 以往的研究主要使用原始特征作为输入，包括RGB图像和光流图。

1.原始特征就是指一整张图吗？



> 使用RGB样本对分别用作ME和MaE的模型输入

1.RGB样本对是？

2.MaE是宏表情吗？

3.模型输入是什么？



> RGB图像在表征面部运动，特别是微表情的面部运动方面是不够的。

1.为什么不够？



> 采用GCN作为特征嵌入主干来表征人脸不同区域之间的关系

**1.如何理解这句话？**

采用*图卷积网络（GCN, Graph Convolutional Network）*作为特征嵌入主干来表征人脸不同区域之间的关系是一种新颖而有效的方法，特别是在人脸识别、表情识别、以及人脸属性分析等任务中。GCN能够自然地处理图结构数据，通过节点之间的连接关系（例如人脸不同区域之间的关系）来捕捉全局和局部信息。

为什么选择GCN来表征人脸不同区域的关系：

1. 自然的图结构表示：
   - 人脸的不同区域可以看作图中的节点，例如眼睛、鼻子、嘴巴等区域，节点之间的边表示这些区域的关系或相互影响。GCN能够直接在这种图结构上进行运算，捕捉到这些区域之间的复杂关系。
2. 局部和全局特征的融合：
   - GCN通过在图结构上卷积操作，能够同时融合局部特征（如单个区域的特征）和全局特征（如整个脸部的整体特征），从而更好地表征人脸的复杂结构和动态变化。
3. 捕捉非欧几里得空间中的关系：
   - 与传统的卷积神经网络（CNN）不同，GCN适合处理非欧几里得空间中的数据关系。在人脸分析中，这种方法可以更灵活地表示不同区域之间的空间关系，而不受固定网格结构的限制。

具体实现：

1. 构建图结构：
   - 将人脸划分为若干关键区域，每个区域作为一个节点。可以通过先验知识（如人脸关键点的分布）或学习的方式定义这些节点之间的连接关系（边）。
2. 特征提取：
   - 使用传统的CNN提取每个节点（人脸区域）的初始特征，然后通过GCN进行特征嵌入，逐层卷积将节点特征与其相邻节点的特征进行融合，捕捉到区域之间的关系。
3. 关系建模：
   - 通过GCN的多层传播机制，可以建模人脸不同区域之间的关系，最终获得一个包含了全局上下文信息的特征表示。这个特征表示可以用于后续的人脸识别或其他任务。
4. 分类或回归：
   - 最后，将GCN输出的特征嵌入输入到一个分类器或回归模型中，用于完成具体的任务，如人脸识别、表情分类等。

优点：

- 灵活性：GCN不依赖固定的网格结构，能够处理任意图结构的数据，这使得它在处理复杂人脸关系时非常有效。
- 增强关系表达：通过图结构捕捉区域之间的关系，GCN能够表征区域间的相互影响，这在一些涉及微妙特征变化的任务中（如表情识别）特别有用。

挑战：

- 计算复杂度：GCN的计算量随着图的复杂度增加而上升，特别是在节点数量多或者图的稀疏度较低时。
- 模型设计复杂性：如何有效构建图结构和设计GCN的架构是一个挑战，特别是在没有明确的先验知识来定义节点和边时。

通过利用GCN来表征人脸不同区域之间的关系，可以更好地捕捉人脸的结构和特征，从而提升模型在相关任务中的表现。

**2.具体如何嵌入？**



> 修改ABPN用于时间特征交互，以更好地利用视频中的上下文信息

**1.如何理解这句话？**

要修改*自适应边界特征网络（ABPN，Adaptive Boundary Prediction Network）*以更好地利用视频中的时间特征交互，可以从以下几个方面进行调整和扩展。目标是让ABPN不仅能够处理静态图像的边界特征，还能在视频序列中有效捕捉和利用时间上下文信息，以增强模型在视频处理任务（如视频分割、动作识别、视频超分辨率等）中的表现。

1. 引入时序卷积或循环神经网络模块

- 时序卷积网络（TCN）：将时序卷积（Temporal Convolutional Network, TCN）引入到ABPN的架构中，使用1D卷积在时间维度上进行滑动，来捕捉短时间范围内的上下文信息。TCN具有并行性强和稳定性的优点，适合处理长时间依赖问题。
- 循环神经网络（RNN）：如果需要更强的时间依赖性，可以考虑在ABPN中嵌入循环神经网络（如LSTM或GRU）。这些模块可以记忆长时间跨度的上下文信息，使得模型能够更好地捕捉视频中随时间变化的动态特征。

2. 时空注意力机制

- 时空注意力（Spatio-Temporal Attention）：增加时空注意力机制，使得模型能够动态关注视频帧中的关键区域和关键时间点。这种机制可以增强模型在捕捉视频中重要特征的能力。具体做法是将空间和时间维度联合起来进行注意力计算，使得ABPN能够自适应地调整对不同时刻和区域的关注度。

3. 多尺度时间特征提取

- 多尺度时序模块：引入多尺度时间特征提取模块，以捕捉不同时间尺度下的视频动态。通过多层次的时间卷积或池化操作，模型可以在不同的时间尺度上提取特征，从而更好地理解视频中既短暂又长期的动态信息。

4. 融合空间和时间特征

- 时空特征融合：在ABPN中增加融合模块，将时间特征与空间特征进行有效融合。可以采用双流网络（Two-Stream Network）架构，一条流处理空间特征，另一条流处理时间特征，最后通过融合层将两者结合起来。这种方法可以增强模型对时间上下文和空间边界的联合理解。

5. 视频特征增强模块

- 残差连接与增强模块：在ABPN中引入残差连接，或者专门设计的特征增强模块，增强时间特征的传递与表达。特别是针对视频中容易发生快速变化的场景，残差连接可以帮助保持特征的传递并减少梯度消失的问题。

6. 时间一致性约束

- 时间一致性正则化：通过增加时间一致性正则化项，使得模型在时间维度上保持输出的一致性。这种正则化可以确保模型在相邻视频帧间的预测具有连贯性，避免不合理的突然变化。

7. 联合训练与多任务学习

- 多任务学习：将ABPN与其他任务（如动作识别、帧间差异检测）联合训练，通过共享时空特征来提升模型对视频上下文信息的理解能力。多任务学习可以有效利用不同任务之间的关联，提升整体性能。

实现策略：

- 模型架构调整：在ABPN中增加上述的时序模块或时空注意力机制，调整模型的输入和输出结构以适应视频数据。
- 数据增强与训练策略：采用视频数据进行训练，并使用帧间数据增强策略（如随机时间剪辑、时序反转等）来提升模型对时间特征的鲁棒性。
- 实验与调优：通过实验确定最佳的模型架构和超参数设置，尤其是在时空特征提取与融合方面，可能需要反复试验以找到最优的组合。

通过这些修改，ABPN可以更好地利用视频中的时间上下文信息，从而在各种视频处理任务中取得更优的性能表现。

**2.具体如何修改？**



> 为了注入面部表情的先验信息，缓解小数据集带来的过拟合问题，我们提出将AU统计信息编码到GCN中，以更好地捕捉面部表情的运动模式，实现更精细的特征表示

**1.如何理解这句话？**



### 实验

> 根据MEGC2021[27]的协议，我们在实验中采用了Leave-One-Subject-Out (LOSO)交叉验证策略。

**1.LOSO协议是什么？**

LOSO协议（Leave-One-Subject-Out）是一种交叉验证策略，主要用于小样本数据集，尤其是在个体差异较大的数据集上，如医学影像分析或脑电图（EEG）等生物医学信号处理中。其核心思想是将数据集中每个个体（或受试者）的数据依次作为验证集，剩余个体的数据作为训练集进行模型训练和验证。

LOSO协议的步骤：

1.划分数据集：对于一个包含N个个体的数据集，先从中选择一个个体的数据作为验证集。

2.训练模型：使用剩余N-1个个体的数据训练模型。

3.验证模型：将训练好的模型在选定的验证集上进行验证，记录其性能指标。

4.重复步骤：重复上述步骤N次，每次选择不同的个体作为验证集，其余作为训练集 。

5.计算平均性能：最终，将所有N次验证的性能指标取平均值，作为模型的总体性能。

LOSO的优点：

* 个体间差异考虑：可以有效考虑个体间的差异，特别适用于个体差异较大的数据集。
* 充分利用数据：相比于传统的*k折交叉验证*，LOSO使得每个个体的数据都可以被用作验证集和训练集。

LOSO的缺点：

* 计算开销大：对于大数据集或包含大量个体的数据集，进行多次训练和验证的计算开销较大。
* 模型不稳定：如果某些个体的数据与其他个体的数据有很大差异，可能会导致模型在某些折中的表现不佳。

这种方法常用于需要精细调优的领域，如医学、心理学和生物医学信号处理等。

*k折交叉验证（k-fold cross-validation）*是一种常用的模型验证方法，用于评估机器学习模型的性能和泛化能力。它通过将数据集分成k个相同大小的子集（或称为“折”），来多次训练和验证模型，以减少由于随机分割数据集导致的评估结果不稳定性。

k折交叉验证的步骤：

1.划分数据集：将整个数据集随机分成k个不重叠的子集，通常k的值为5或10。

2.迭代训练和验证：

* 在第i次迭代中，选择第i个子集作为验证集，其他k-1个子集作为训练集。
* 使用训练集训练模型。
* 使用验证集评估模型的性能，并记录性能指标（如准确率、精确率、召回率等）。

3.重复k次：重复上述步骤，共进行k次迭代，每次选择不同的子集作为验证集。

4.计算平均性能：将k次迭代的性能指标取平均值，作为模型的总体性能。

k折交叉验证的优点：

* 减少偏差：通过多次训练和验证，k折交叉验证减少了由于数据随机分割导致的结果偏差。
* 充分利用数据：与简单的训练/验证数据集 划分方法相比，k折交叉验证能够更有效地利用数据集中的每一个样本。
* 适用于小数据集：在数据集较小的情况下，这种方法尤其有用，因为它能够最大限度地利用有限的数据。

k折交叉验证的缺点：

* 计算开销：k折交叉验证需要进行k次训练和验证，计算量较大，特别是对于大数据集或复杂模型。
* 不适用于时间序列数据：对于时间序列数据，这种方法可能不合适，因为它打乱了数据的实践顺序，无法保持时间依赖性。

特殊形式：

* 留一法（LOOCV）：k折交叉验证的一种极端情况，当k等于数据集的样本数时，这种方法称为留一法（Leave-One-Out Cross-Validation,LOOCV）。每次只留一个样本作为验证集，其余样本作为训练集。

k折交叉验证是机器学习中广泛应用的一种验证方法，它能够提供关于模型泛化能力的较为可靠的估计。

来源：ChatGPT

## 论文重点分析

1.为什么使用数据集时没有使用RGB图像？使用的是npz文件？是否可以用其他数据集替代？

2.如何改进？

3.为什么数据集的编号是从015开始，而不是从0开始？之前的数据不需要吗？



## 阅读感想

1.训练模型时，不是直接使用的图片和视频，而是使用npz数组。

2.项目中交代了面部关键点和兴趣区域的提取的具体方法

## 代码分析

1.使用两个数据集CAS(ME)^2和SAMM-LV，代码中是分别训练，但是在在最终结果统计中，综合统计了两个数据集的训练效果，~~在代码中有`cross.csv`这样类似的文件~~（在项目中发现cross是一个数据集），猜测是否使用两个数据集进行交叉训练和验证。



2.代码读入数据集时没有直接读取图片和视频，使用的是处理好的数据集，形式上是分块的`npz`文件，猜测是最初使用图片和视频以及AU信息进行处理之后形成的`npz`文件，虽然在项目中存在相关的面部信息处理的代码文件，但不能确定是否是从源图片和视频处进行处理。如果从源图片和视频进行处理，但是没从代码文件中找到相关读取图片和视频的路径。

3.为什么待处理的数据集命名不从0开始，比如casme从015开始，samm从007开始？猜测只处理视频，在项目给出的图片中的数据预处理部分，给出的是输入视频，但是还未进行验证。

4.代码的输出结果没有输出图片一类，可能需要自己写一部分代码，将输出的结果使用曲线图或折线图进行表示。

5.输出结果中还有`pth`一类的文件，但是是分块的，一个数据集有多个`pth`文件。一般认为跑一次代码后获得的模型，下一次跑代码时可以使用上一次的模型，但是在代码中没找到类似导入之前的模型的代码，不过找到数据预处理的一个函数，这个函数中的读入的路径是项目中不存在的路径。所以是否需要自己写相关的读入上次训练的模型的代码？



