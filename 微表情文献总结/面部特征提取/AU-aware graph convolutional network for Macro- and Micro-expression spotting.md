# AU-aware graph convolutional network for Macro- and Micro-expression spotting

基于AU感知的宏表情和微表情识别的图卷积网络  

DOI:10.48550/arXiv.2303.09114  

## 论文详细分析

### 摘要

> 建模面部感兴趣区域（ROI）之间的关系来提取更精细的空间特征

1.如何建模建立关系？  

2.什么是空间特征？  

参考:

[GCN—图卷积神经网络理解_gcn提取空间特征-CSDN博客](https://blog.csdn.net/wsp_1138886114/article/details/100709692)

[机器学习术语表 | Freeopen](https://freeopen.github.io/glossary/)

这里的空间特征是机器学习中的概念。

对于图片而言，中心点像素及其邻域像素构成空间，这些像素的值和关系构成特征。




> 提出了一个基于图卷积的网络，成为动作单元感知图卷积网络（AUW-GCN）

1.什么是图卷积？  

参考：

[GCN—图卷积神经网络理解_gcn提取空间特征-CSDN博客](https://blog.csdn.net/wsp_1138886114/article/details/100709692)

卷积一般指离散卷积，其本质是加权求和。

通过计算中心像素点以及相邻像素点的加权和来构成特征图谱以实现空间特征的提取。

传统的CNN（卷积神经网络）处理的图像或者视频的像素点是排列的很整齐的矩阵，但实际上有许多数据及其关系不是规整的矩阵，其网络结构为拓扑图，而GCN就是用于处理拓扑结构，用于弥补CNN无法处理拓扑图的缺陷。

图卷积是将一个节点周围的邻居按照不同的权重叠加起来，每个节点的周围的的邻居数不固定。但是一般卷积的节点的邻居数是固定的。

2.什么是基于图卷积的网络？  

参考：

[图神经网络之图卷积网络——GCN_图卷积的神经网络-CSDN博客](https://blog.csdn.net/zbp_12138/article/details/110246797)

有一些公式不太理解。

图卷积相比图像卷积更灵活。



> 为了注入先验信息和解决小数据集的问题，将AU相关的统计信息编码到网络中

1.什么是注入先验信息？先验信息是什么？

参考：

[如何将先验知识注入推荐模型-CSDN博客](https://blog.csdn.net/Kaiyuan_sjtu/article/details/121987636)

是不是预处理模型？

2.小数据集是？

参考：

[【干货指南】机器学习必须需要大量数据？小数据集也能有大价值！ (qq.com)](https://mp.weixin.qq.com/s/xGnDcRtKKt4FyVRAMPSqYA)

3.如何将AU的统计信息编码到网络中？  

参考：

[【论文阅读】AU检测|《Deep Adaptive Attention for Joint Facial Action Unit Detection and Face Alignment》_au 关键点检测-CSDN博客](https://blog.csdn.net/qq_43521527/article/details/113440970)

不太能理解。



> 在CAS(ME)2和SAMM-LV两个基准数据集上取得了新的SOTA性能

1.SOTA性能是什么?  

参考：

[论文里SOTA性能的理解-CSDN博客](https://blog.csdn.net/wuyeyoulan23/article/details/123756127)

大概是最好、最先进的意思。



### 引言

> 微表情研究的一个重要部分是微表情的定位（发现），包括在未剪辑的长视频中定位表情间隔

1.如何定位?



> 以往的研究主要使用原始特征作为输入，包括RGB图像和光流图。

1.原始特征就是指一整张图吗？



> 使用RGB样本对分别用作ME和MaE的模型输入

1.RGB样本对是？

2.MaE是宏表情吗？

3.模型输入是什么？



> RGB图像在表征面部运动，特别是微表情的面部运动方面是不够的。

1.为什么不够？



### 实验

> 根据MEGC2021[27]的协议，我们在实验中采用了Leave-One-Subject-Out (LOSO)交叉验证策略。

**1.LOSO协议是什么？**

LOSO协议（Leave-One-Subject-Out）是一种交叉验证策略，主要用于小样本数据集，尤其是在个体差异较大的数据集上，如医学影像分析或脑电图（EEG）等生物医学信号处理中。其核心思想是将数据集中每个个体（或受试者）的数据依次作为验证集，剩余个体的数据作为训练集进行模型训练和验证。

LOSO协议的步骤：

1.划分数据集：对于一个包含N个个体的数据集，先从中选择一个个体的数据作为验证集。

2.训练模型：使用剩余N-1个个体的数据训练模型。

3.验证模型：将训练好的模型在选定的验证集上进行验证，记录其性能指标。

4.重复步骤：重复上述步骤N次，每次选择不同的个体作为验证集，其余作为训练集 。

5.计算平均性能：最终，将所有N次验证的性能指标取平均值，作为模型的总体性能。

LOSO的优点：

* 个体间差异考虑：可以有效考虑个体间的差异，特别适用于个体差异较大的数据集。
* 充分利用数据：相比于传统的*k折交叉验证*，LOSO使得每个个体的数据都可以被用作验证集和训练集。

LOSO的缺点：

* 计算开销大：对于大数据集或包含大量个体的数据集，进行多次训练和验证的计算开销较大。
* 模型不稳定：如果某些个体的数据与其他个体的数据有很大差异，可能会导致模型在某些折中的表现不佳。

这种方法常用于需要精细调优的领域，如医学、心理学和生物医学信号处理等。

*k折交叉验证（k-fold cross-validation）*是一种常用的模型验证方法，用于评估机器学习模型的性能和泛化能力。它通过将数据集分成k个相同大小的子集（或称为“折”），来多次训练和验证模型，以减少由于随机分割数据集导致的评估结果不稳定性。

k折交叉验证的步骤：

1.划分数据集：将整个数据集随机分成k个不重叠的子集，通常k的值为5或10。

2.迭代训练和验证：

* 在第i次迭代中，选择第i个子集作为验证集，其他k-1个子集作为训练集。
* 使用训练集训练模型。
* 使用验证集评估模型的性能，并记录性能指标（如准确率、精确率、召回率等）。

3.重复k次：重复上述步骤，共进行k次迭代，每次选择不同的子集作为验证集。

4.计算平均性能：将k次迭代的性能指标取平均值，作为模型的总体性能。

k折交叉验证的优点：

* 减少偏差：通过多次训练和验证，k折交叉验证减少了由于数据随机分割导致的结果偏差。
* 充分利用数据：与简单的训练/验证数据集 划分方法相比，k折交叉验证能够更有效地利用数据集中的每一个样本。
* 适用于小数据集：在数据集较小的情况下，这种方法尤其有用，因为它能够最大限度地利用有限的数据。

k折交叉验证的缺点：

* 计算开销：k折交叉验证需要进行k次训练和验证，计算量较大，特别是对于大数据集或复杂模型。
* 不适用于时间序列数据：对于时间序列数据，这种方法可能不合适，因为它打乱了数据的实践顺序，无法保持时间依赖性。

特殊形式：

* 留一法（LOOCV）：k折交叉验证的一种极端情况，当k等于数据集的样本数时，这种方法称为留一法（Leave-One-Out Cross-Validation,LOOCV）。每次只留一个样本作为验证集，其余样本作为训练集。

k折交叉验证是机器学习中广泛应用的一种验证方法，它能够提供关于模型泛化能力的较为可靠的估计。

来源：ChatGPT

## 论文重点分析

1.为什么使用数据集时没有使用RGB图像？使用的是npz文件？是否可以用其他数据集替代？

2.如何改进？

3.为什么数据集的编号是从015开始，而不是从0开始？之前的数据不需要吗？
