# AU-aware graph convolutional network for Macro- and Micro-expression spotting

基于AU感知的宏表情和微表情识别的图卷积网络  

DOI:10.48550/arXiv.2303.09114  

## 论文详细分析

### 摘要

> 建模面部感兴趣区域（ROI）之间的关系来提取更精细的空间特征

1.如何建模建立关系？  

2.什么是空间特征？  

参考:

[GCN—图卷积神经网络理解_gcn提取空间特征-CSDN博客](https://blog.csdn.net/wsp_1138886114/article/details/100709692)

[机器学习术语表 | Freeopen](https://freeopen.github.io/glossary/)

这里的空间特征是机器学习中的概念。

对于图片而言，中心点像素及其邻域像素构成空间，这些像素的值和关系构成特征。




> 提出了一个基于图卷积的网络，成为动作单元感知图卷积网络（AUW-GCN）

1.什么是图卷积？  

参考：

[GCN—图卷积神经网络理解_gcn提取空间特征-CSDN博客](https://blog.csdn.net/wsp_1138886114/article/details/100709692)

卷积一般指离散卷积，其本质是加权求和。

通过计算中心像素点以及相邻像素点的加权和来构成特征图谱以实现空间特征的提取。

传统的CNN（卷积神经网络）处理的图像或者视频的像素点是排列的很整齐的矩阵，但实际上有许多数据及其关系不是规整的矩阵，其网络结构为拓扑图，而GCN就是用于处理拓扑结构，用于弥补CNN无法处理拓扑图的缺陷。

图卷积是将一个节点周围的邻居按照不同的权重叠加起来，每个节点的周围的的邻居数不固定。但是一般卷积的节点的邻居数是固定的。

2.什么是基于图卷积的网络？  

参考：

[图神经网络之图卷积网络——GCN_图卷积的神经网络-CSDN博客](https://blog.csdn.net/zbp_12138/article/details/110246797)

有一些公式不太理解。

图卷积相比图像卷积更灵活。



> 为了注入先验信息和解决小数据集的问题，将AU相关的统计信息编码到网络中

1.什么是注入先验信息？先验信息是什么？

参考：

[如何将先验知识注入推荐模型-CSDN博客](https://blog.csdn.net/Kaiyuan_sjtu/article/details/121987636)

是不是预处理模型？

2.小数据集是？

参考：

[【干货指南】机器学习必须需要大量数据？小数据集也能有大价值！ (qq.com)](https://mp.weixin.qq.com/s/xGnDcRtKKt4FyVRAMPSqYA)

3.如何将AU的统计信息编码到网络中？  

参考：

[【论文阅读】AU检测|《Deep Adaptive Attention for Joint Facial Action Unit Detection and Face Alignment》_au 关键点检测-CSDN博客](https://blog.csdn.net/qq_43521527/article/details/113440970)

不太能理解。



> 在CAS(ME)2和SAMM-LV两个基准数据集上取得了新的SOTA性能

1.SOTA性能是什么?  

参考：

[论文里SOTA性能的理解-CSDN博客](https://blog.csdn.net/wuyeyoulan23/article/details/123756127)

大概是最好、最先进的意思。



### 引言

> 微表情研究的一个重要部分是微表情的定位（发现），包括在未剪辑的长视频中定位表情间隔

1.如何定位?



> 以往的研究主要使用原始特征作为输入，包括RGB图像和光流图。

1.原始特征就是指一整张图吗？



> 使用RGB样本对分别用作ME和MaE的模型输入

1.RGB样本对是？

2.MaE是宏表情吗？

3.模型输入是什么？



> RGB图像在表征面部运动，特别是微表情的面部运动方面是不够的。

1.为什么不够？



> 采用GCN作为特征嵌入主干来表征人脸不同区域之间的关系

采用*图卷积网络（GCN, Graph Convolutional Network）*作为特征嵌入主干来表征人脸不同区域之间的关系是一种新颖而有效的方法，特别是在人脸识别、表情识别、以及人脸属性分析等任务中。GCN能够自然地处理图结构数据，通过节点之间的连接关系（例如人脸不同区域之间的关系）来捕捉全局和局部信息。

为什么选择GCN来表征人脸不同区域的关系：

1. 自然的图结构表示：
   - 人脸的不同区域可以看作图中的节点，例如眼睛、鼻子、嘴巴等区域，节点之间的边表示这些区域的关系或相互影响。GCN能够直接在这种图结构上进行运算，捕捉到这些区域之间的复杂关系。
2. 局部和全局特征的融合：
   - GCN通过在图结构上卷积操作，能够同时融合局部特征（如单个区域的特征）和全局特征（如整个脸部的整体特征），从而更好地表征人脸的复杂结构和动态变化。
3. 捕捉非欧几里得空间中的关系：
   - 与传统的卷积神经网络（CNN）不同，GCN适合处理非欧几里得空间中的数据关系。在人脸分析中，这种方法可以更灵活地表示不同区域之间的空间关系，而不受固定网格结构的限制。

具体实现：

1. 构建图结构：
   - 将人脸划分为若干关键区域，每个区域作为一个节点。可以通过先验知识（如人脸关键点的分布）或学习的方式定义这些节点之间的连接关系（边）。
2. 特征提取：
   - 使用传统的CNN提取每个节点（人脸区域）的初始特征，然后通过GCN进行特征嵌入，逐层卷积将节点特征与其相邻节点的特征进行融合，捕捉到区域之间的关系。
3. 关系建模：
   - 通过GCN的多层传播机制，可以建模人脸不同区域之间的关系，最终获得一个包含了全局上下文信息的特征表示。这个特征表示可以用于后续的人脸识别或其他任务。
4. 分类或回归：
   - 最后，将GCN输出的特征嵌入输入到一个分类器或回归模型中，用于完成具体的任务，如人脸识别、表情分类等。

优点：

- 灵活性：GCN不依赖固定的网格结构，能够处理任意图结构的数据，这使得它在处理复杂人脸关系时非常有效。
- 增强关系表达：通过图结构捕捉区域之间的关系，GCN能够表征区域间的相互影响，这在一些涉及微妙特征变化的任务中（如表情识别）特别有用。

挑战：

- 计算复杂度：GCN的计算量随着图的复杂度增加而上升，特别是在节点数量多或者图的稀疏度较低时。
- 模型设计复杂性：如何有效构建图结构和设计GCN的架构是一个挑战，特别是在没有明确的先验知识来定义节点和边时。

通过利用GCN来表征人脸不同区域之间的关系，可以更好地捕捉人脸的结构和特征，从而提升模型在相关任务中的表现。



> 修改ABPN用于时间特征交互，以更好地利用视频中的上下文信息

**1.如何理解这句话？**

要修改*自适应边界特征网络（ABPN，Adaptive Boundary Prediction Network）*以更好地利用视频中的时间特征交互，可以从以下几个方面进行调整和扩展。目标是让ABPN不仅能够处理静态图像的边界特征，还能在视频序列中有效捕捉和利用时间上下文信息，以增强模型在视频处理任务（如视频分割、动作识别、视频超分辨率等）中的表现。

1. 引入时序卷积或循环神经网络模块

- 时序卷积网络（TCN）：将时序卷积（Temporal Convolutional Network, TCN）引入到ABPN的架构中，使用1D卷积在时间维度上进行滑动，来捕捉短时间范围内的上下文信息。TCN具有并行性强和稳定性的优点，适合处理长时间依赖问题。
- 循环神经网络（RNN）：如果需要更强的时间依赖性，可以考虑在ABPN中嵌入循环神经网络（如LSTM或GRU）。这些模块可以记忆长时间跨度的上下文信息，使得模型能够更好地捕捉视频中随时间变化的动态特征。

2. 时空注意力机制

- 时空注意力（Spatio-Temporal Attention）：增加时空注意力机制，使得模型能够动态关注视频帧中的关键区域和关键时间点。这种机制可以增强模型在捕捉视频中重要特征的能力。具体做法是将空间和时间维度联合起来进行注意力计算，使得ABPN能够自适应地调整对不同时刻和区域的关注度。

3. 多尺度时间特征提取

- 多尺度时序模块：引入多尺度时间特征提取模块，以捕捉不同时间尺度下的视频动态。通过多层次的时间卷积或池化操作，模型可以在不同的时间尺度上提取特征，从而更好地理解视频中既短暂又长期的动态信息。

4. 融合空间和时间特征

- 时空特征融合：在ABPN中增加融合模块，将时间特征与空间特征进行有效融合。可以采用双流网络（Two-Stream Network）架构，一条流处理空间特征，另一条流处理时间特征，最后通过融合层将两者结合起来。这种方法可以增强模型对时间上下文和空间边界的联合理解。

5. 视频特征增强模块

- 残差连接与增强模块：在ABPN中引入残差连接，或者专门设计的特征增强模块，增强时间特征的传递与表达。特别是针对视频中容易发生快速变化的场景，残差连接可以帮助保持特征的传递并减少梯度消失的问题。

6. 时间一致性约束

- 时间一致性正则化：通过增加时间一致性正则化项，使得模型在时间维度上保持输出的一致性。这种正则化可以确保模型在相邻视频帧间的预测具有连贯性，避免不合理的突然变化。

7. 联合训练与多任务学习

- 多任务学习：将ABPN与其他任务（如动作识别、帧间差异检测）联合训练，通过共享时空特征来提升模型对视频上下文信息的理解能力。多任务学习可以有效利用不同任务之间的关联，提升整体性能。

实现策略：

- 模型架构调整：在ABPN中增加上述的时序模块或时空注意力机制，调整模型的输入和输出结构以适应视频数据。
- 数据增强与训练策略：采用视频数据进行训练，并使用帧间数据增强策略（如随机时间剪辑、时序反转等）来提升模型对时间特征的鲁棒性。
- 实验与调优：通过实验确定最佳的模型架构和超参数设置，尤其是在时空特征提取与融合方面，可能需要反复试验以找到最优的组合。

通过这些修改，ABPN可以更好地利用视频中的时间上下文信息，从而在各种视频处理任务中取得更优的性能表现。



> 为了注入面部表情的先验信息，缓解小数据集带来的过拟合问题，我们提出将AU统计信息编码到GCN中，以更好地捕捉面部表情的运动模式，实现更精细的特征表示

将*面部动作单元（AU，Action Unit）统计信息编码到图卷积网络（GCN）*中，以捕捉面部表情的运动模式，确实是一个有效的策略，尤其是在处理小数据集时，这种方法能够引入先验知识，从而缓解过拟合问题，并实现更精细的特征表示。以下是如何实现这一策略的具体方法：

1. AU统计信息的提取与编码

- AU提取：使用现有的AU检测模型（如基于卷积神经网络的AU检测器）来提取每个面部表情中的AU信息。AU描述了面部表情中肌肉的运动，例如眉毛抬起、嘴角上扬等。这些信息可以作为面部表情的运动模式的低级特征。
- 统计信息编码：将提取的AU信息进行统计处理，如计算频率、共现模式（多个AU同时出现的频率），或者通过直方图、向量等方式对这些统计信息进行编码，生成可以作为输入的特征表示。

2. 图结构的构建与AU信息嵌入

- 节点表示：在GCN中，每个节点可以表示面部的一个特定区域（如眼睛、嘴巴、额头等），这些节点的初始特征可以包括原始的视觉特征（如通过CNN提取的特征），并结合AU的统计信息。
- 边的定义：基于面部解剖学知识或AU的共现关系来定义节点之间的边。例如，如果两个AU经常一起激活，可以在对应的区域节点之间增加连接边，从而捕捉到这些区域的协同运动。

3. GCN中的AU信息融合

- 特征融合：将AU统计信息与传统的视觉特征进行融合，作为节点特征的输入。可以通过特征拼接、加权和或使用注意力机制来进行融合，从而使模型既能捕捉到空间结构特征，也能理解AU层面的运动模式。
- 时序信息融合：如果处理的是视频数据，还可以将AU信息在时间维度上进行编码，通过时序GCN或与RNN结合的方式，捕捉面部表情的动态变化。

4. 训练与正则化策略

- 先验约束：利用AU信息作为先验，在训练过程中引入正则化项，强制模型学习与AU相关的特征，以提高对小数据集的泛化能力。例如，可以在损失函数中加入与AU一致性的正则化项，促使模型输出与AU运动模式一致的特征表示。
- 数据增强：在训练时使用基于AU的增强策略，比如模拟不同AU的激活模式，以生成更多样的训练数据，进一步缓解过拟合问题。

5. 精细化的特征表示

- 多任务学习：通过多任务学习，将面部表情识别任务与AU检测任务结合，使得GCN同时优化这两个任务的目标，从而利用AU的先验知识提高面部表情识别的精度。
- 层次化特征表示：在GCN的不同层中逐步整合AU信息，从低层的简单AU特征到高层的复杂表情模式，最终实现更加精细和语义丰富的特征表示。

6. 实验验证与调优

- 模型验证：在小数据集上进行交叉验证，验证模型是否通过引入AU信息取得了更好的泛化性能。
- 超参数调优：调整AU信息嵌入的方式和GCN的结构参数，如节点数、层数、学习率等，以获得最优的模型性能。

实施效果

通过将AU统计信息编码到GCN中，模型可以利用面部表情的运动模式来实现更精细的特征表示，从而在小数据集上有效缓解过拟合问题，提升模型的性能和泛化能力。



### 相关工作

#### 微表情的定位

> 研究人员会选择一个标准的架构，包括cnn和rnn，并将任务视为一个典型的监督问题。

研究人员在处理视频分析、序列预测或时序数据任务时，通常会选择一个标准的深度学习架构，包括卷积神经网络（CNN）和循环神经网络（RNN），并将任务视为一个典型的监督学习问题。这种方法在许多领域得到了广泛应用，以下是其典型架构和操作流程：

**标准架构选择：CNN + RNN**

1. **卷积神经网络（CNN）**：
   - **功能**：CNN主要用于提取输入数据（如图像或视频帧）的空间特征。对于图像数据，CNN能够识别出局部的空间模式，如边缘、纹理、形状等。这些空间特征在时序任务中也同样重要，因为每一帧的空间特征可以反映物体或场景的瞬时状态。
   - **应用**：在视频处理任务中，通常会对每一帧使用CNN进行特征提取，得到一系列表示每帧空间信息的特征向量。
2. **循环神经网络（RNN）**：
   - **功能**：RNN擅长处理序列数据，能够捕捉输入数据中的时间依赖性。通过在序列中一步一步地处理数据，RNN可以记忆前一时刻的信息，并结合当前输入进行决策。
   - **变体**：为了更好地处理长时间依赖，通常会使用RNN的变体，如长短期记忆网络（LSTM）或门控循环单元（GRU）。这些变体能够在较长的时间跨度内保留信息，避免梯度消失问题。
   - **应用**：将通过CNN提取的每一帧的特征向量输入RNN中，RNN将这些序列特征进行处理，以捕捉时间上的动态变化，从而获得整个视频的全局时序特征。

**监督学习任务**

1. **任务定义**：
   - **输入**：视频序列或时序数据，通常是多帧图像或一系列时间步的数据。
   - **输出**：模型需要预测的目标，例如视频中的某个动作类别、序列中的下一个值、或者视频帧之间的关联等。
2. **损失函数**：
   - 在监督学习框架下，研究人员会定义一个损失函数，如交叉熵损失（用于分类问题）、均方误差（用于回归问题）等，来衡量模型预测值与真实标签之间的差异。
3. **训练过程**：
   - **前向传播**：数据通过CNN和RNN的组合模型，生成输出预测。
   - **反向传播和梯度更新**：根据损失函数的反馈，使用反向传播算法计算梯度，并通过优化器（如SGD、Adam）更新模型参数。
   - **迭代优化**：通过多次迭代，逐步减小损失函数值，从而优化模型性能。
4. **模型评估**：
   - 训练结束后，研究人员会使用验证集或测试集来评估模型的性能。常用的评估指标包括准确率、精度、召回率、F1分数等。

**适用场景**

- **视频分类**：CNN负责提取每一帧的特征，RNN捕捉帧之间的时序关系，最终输出视频的类别。
- **动作识别**：通过CNN提取的空间特征和RNN捕捉的时间特征，识别视频中的动作类型。
- **序列预测**：在时序数据分析中，如金融时间序列、天气预测，CNN可以用于局部模式的检测，RNN则用于预测未来的序列值。

**优点与局限性**

- **优点**：这种架构能够同时利用空间和时间特征，适合处理时空数据，并且结构相对简单，易于实现和调优。
- **局限性**：由于CNN和RNN的组合可能带来较大的计算开销，特别是在长序列或高分辨率图像处理中。此外，传统RNN在处理长序列时可能存在效率问题，尽管LSTM和GRU已经有所改善。

**扩展与改进**

- 研究人员也可以结合更复杂的模型架构，如在CNN-RNN基础上引入注意力机制，或者采用更先进的模型如Transformer，以进一步提高对长时间依赖或复杂场景的建模能力。

这种标准的CNN+RNN架构是解决视频和时序任务的经典方法，特别是在处理复杂数据时，它为特征提取和时序建模提供了有效的解决方案。



> 分别使用2D-CNNs和1D-CNNs提取空间特征和时间特征，并使用回归网络来细化区间

将2D-CNN和1D-CNN结合起来分别提取空间特征和时间特征，然后使用回归网络来细化预测区间的策略是一种有效的多阶段特征处理方法，尤其适用于视频分析、时序数据处理等任务。下面是具体的实现步骤和架构设计：

1. **2D-CNNs 提取空间特征**

- **输入**：从视频中提取的单帧图像或图像片段。
- 2D-CNN架构：
  - 使用标准的2D卷积神经网络（例如ResNet、VGG、MobileNet等）对每一帧图像进行处理。2D-CNN适用于图像的空间特征提取，如物体边缘、纹理、形状等。
  - 经过卷积、池化、激活函数等操作，提取每一帧的高维空间特征。最终输出的特征向量代表该帧图像的空间特征。

2. **1D-CNNs 提取时间特征**

- **输入**：经过2D-CNN处理后的帧特征序列。
- 1D-CNN架构：
  - 1D卷积网络用于处理时序数据，专门提取帧之间的时间依赖特征。输入到1D-CNN的特征序列的每一个时间步对应一帧图像的空间特征。
  - 1D-CNN通过在时间维度上进行卷积操作，捕捉到序列中连续帧之间的动态关系，如运动轨迹、帧间变化等。
  - 1D-CNN输出的特征向量代表整个视频片段的时间特征，结合了帧与帧之间的时间信息。

3. **回归网络细化区间**

- **输入**：从1D-CNN提取出的时间特征。
- 回归网络架构：
  - 回归网络通常由若干全连接层（Fully Connected Layer）组成，其作用是对时间特征进行进一步的处理和细化，用于预测一个连续值或一个时间区间。
  - 回归网络的输出可以是用于区间预测的具体值，如动作起始时间和结束时间，或者某种连续变量的估计。
  - 可以通过损失函数（如均方误差MSE）来优化回归网络，使得预测值更加接近真实值，从而细化和提高区间预测的精度。

4. **整体架构总结**

1. **2D-CNN**：提取每一帧图像的空间特征，生成特征向量序列。
2. **1D-CNN**：在时间维度上处理特征向量序列，提取视频或时序数据的时间特征。
3. **回归网络**：利用时间特征进行区间预测和细化。

5. **训练和优化**

- **损失函数**：根据具体任务选择合适的损失函数。例如，区间预测的回归任务可以使用均方误差（MSE）作为损失函数。
- **优化算法**：使用优化算法如Adam或SGD来进行参数更新，确保网络在训练集上的表现逐步提高，并且能够在验证集上具有良好的泛化能力。
- **数据增强**：在2D-CNN部分，可以对输入图像进行各种增强操作（如旋转、缩放、裁剪）以提高模型的鲁棒性。在1D-CNN部分，可以使用不同的序列长度和时间间隔来增强时序数据的多样性。

6. **应用场景**

- **视频动作识别**：在动作识别任务中，2D-CNN负责识别每帧的静态动作特征，而1D-CNN捕捉动作随时间的动态变化。回归网络可以细化动作的发生时间或预测持续时间。
- **时序数据分析**：适用于金融市场分析、传感器数据处理等领域，2D-CNN可以处理多维特征（如不同传感器的读数），1D-CNN分析时序变化，回归网络细化预测区间。

通过这种多阶段的特征提取和细化策略，能够更精确地捕捉到视频或时序数据中的复杂模式，从而提高整体模型的预测能力和表现。

> 采用3D-CNNs同时提取时空特征

采用3D卷积神经网络（3D-CNNs）同时提取时空特征是一种强大而有效的方法，特别适用于处理视频数据或其他具有时间和空间维度的序列数据。3D-CNN能够直接在时空域中进行卷积操作，从而同时捕捉到时间和空间特征的相互关系。

1. **3D-CNN的基本原理**

- **3D卷积**：与2D-CNN不同，3D-CNN在输入数据的三个维度（高度、宽度、时间）上进行卷积操作。卷积核是三维的，能够同时在空间和时间维度上滑动，捕捉局部的时空特征。
- **输入数据**：3D-CNN通常接收视频片段作为输入，其中视频片段的每一帧组成一个时间序列。输入张量的维度通常为（帧数 × 高度 × 宽度 × 通道数）。

2. **3D-CNN的架构设计**

- **输入层**：
  - 输入一个视频片段，通常是一个固定长度的时间窗口。例如，输入可以是一个尺寸为（T × H × W × C）的张量，其中T是帧数，H是高度，W是宽度，C是通道数（通常为3，对应RGB图像）。
- **3D卷积层**：
  - 3D卷积核在空间（高度H和宽度W）和时间（帧数T）上同时滑动，从输入数据中提取时空特征。每个卷积操作都考虑了当前帧及其邻近帧的信息。
  - 3D卷积操作能够捕捉到局部时空区域内的运动模式，如物体的运动轨迹、变化趋势等。
- **3D池化层**：
  - 3D池化层（如3D最大池化或平均池化）在卷积层后使用，用于下采样特征图，减小数据维度，同时保留重要的时空信息。
  - 池化操作也在空间和时间维度上同时进行，从而简化后续层的计算量。
- **激活函数**：
  - 常用的激活函数如ReLU应用在每个卷积层的输出之后，增加网络的非线性表达能力。
- **全连接层（FC）**：
  - 在经过多个3D卷积和池化层后，特征图通常被展平成一个向量，并通过全连接层进行最终的分类或回归任务。

3. **应用场景**

- **视频分类**：3D-CNN可以用于视频分类任务，如动作识别、场景分类等。模型能够直接从视频片段中学习到空间和时间的复合特征，从而提高分类精度。
- **动作识别**：通过3D-CNN提取的时空特征，可以准确识别视频中的复杂动作，特别是当动作涉及多个时间步和空间变化时。
- **视频分割**：3D-CNN可以用于视频分割任务，通过捕捉时空特征来确定视频中不同区域的类别或物体边界。
- **时序预测**：适用于任何涉及时空数据的预测任务，如医疗图像分析中的动态变化监测，或交通流量预测。

4. **训练和优化策略**

- 数据增强：
  - 对于视频输入，可以进行各种时空数据增强操作，如随机裁剪、旋转、时间翻转等，以增强模型的泛化能力。
- 正则化：
  - 使用Dropout、权重衰减等正则化技术，避免3D-CNN过拟合，特别是在训练数据有限的情况下。
- 优化器和损失函数：
  - 根据任务选择合适的损失函数（如交叉熵、均方误差）和优化算法（如Adam、SGD）来优化网络。

5. **优点与挑战**

- **优点**：
  - 3D-CNN能够同时提取时间和空间特征，避免了传统方法中将时间和空间特征分开处理的局限性。
  - 直接在时空域操作，使得模型能够捕捉到复杂的时空交互关系，提高模型的识别和预测能力。
- **挑战**：
  - 计算量较大：由于3D卷积核在三个维度上进行操作，计算复杂度比2D卷积高，训练时间和资源需求较大。
  - 需要大量的标注数据：3D-CNN的强大表达能力往往需要大量的标注数据进行训练，否则可能容易过拟合。

6. **实现效果**

- 在许多视频分析任务中，3D-CNN已经展现了其强大的性能，特别是在捕捉时空特征和处理动态场景方面，3D-CNN能够显著提升模型的准确性和稳健性。

采用3D-CNN提取时空特征可以提供比传统方法更丰富的特征表示，使得模型在视频分类、动作识别、视频分割等任务中取得更优的效果。这种方法尤其适用于处理复杂时空关系的任务。

> 将BSN灵活地用于识别微表情和宏表情的间隔

将边界敏感网络（Boundary Sensitive Network，BSN）灵活应用于识别微表情和宏表情的间隔是一个非常具有挑战性但也非常有效的策略。BSN最初设计用于视频动作检测中的边界预测和动作提取，但其原理同样可以扩展到微表情和宏表情的识别任务中，尤其是对于复杂的时空模式识别。

1. **BSN概述**

- **BSN的核心思想**：BSN通过预测视频片段中的边界点来确定动作的起始和结束时间。它主要由三个模块组成：边界生成模块、边界置信度评估模块和提议生成模块。这些模块合作，可以有效地检测视频中的动作区间。
- **应用扩展**：对于微表情和宏表情的检测，我们可以将BSN中的动作边界检测扩展为表情区间检测，微调模型以识别表情变化的起点和终点。

2. **微表情和宏表情的特点**

- **微表情**：通常持续时间非常短（大约0.5秒或更短），表现为细微的面部肌肉变化，具有隐蔽性和快速性。
- **宏表情**：持续时间较长，通常能够被人眼明显观察到，并伴随着明显的面部肌肉活动。

3. **BSN应用于表情识别的策略**

- **输入数据处理**：
  - 视频片段的每一帧输入到网络中，作为基础特征，用于提取面部的时空特征。
  - 使用3D-CNN或2D-CNN + RNN结合的方式，先提取每帧图像的空间特征，然后捕捉视频序列中的时间特征。
- **边界生成模块**：
  - 该模块用于生成潜在的表情区间边界。对于微表情，由于其短暂性，需要较高的时间分辨率来检测细微的变化；对于宏表情，可以使用较长的时间窗口。
  - BSN通过滑动窗口在时间轴上生成候选边界点，并预测每个点的表情变化开始（起点）和结束（终点）的概率。
- **边界置信度评估模块**：
  - 对于每一个候选边界，BSN使用置信度评估模块来计算该边界作为表情变化边界的概率。这个过程涉及特征的进一步提取和分析。
  - 置信度得分可以用来过滤低质量的候选边界点，从而保留最可能的微表情或宏表情区间。
- **提议生成模块**：
  - 将候选边界点进行组合，生成可能的表情区间。BSN会对这些区间进行排序，保留高置信度的区间作为最终的表情识别结果。
  - 提议生成模块可以进一步根据表情的时间长度特征来分类微表情和宏表情。例如，短时间的区间可以标记为微表情，而较长时间的区间则为宏表情。

4. **模型训练与优化**

- 多任务学习：
  - 将微表情和宏表情的检测作为两个互补的任务来进行训练。通过共享特征提取层，模型可以在捕捉细微表情变化的同时，也能准确识别较长时间的宏表情。
- 损失函数：
  - 对于边界检测任务，可以采用边界损失函数，如二元交叉熵损失（Binary Cross-Entropy）来训练边界生成模块和置信度评估模块。
  - 对于区间分类（微表情和宏表情），可以使用分类损失函数，如交叉熵损失。
- 数据增强：
  - 在训练过程中，可以通过模拟不同的表情变化速度和持续时间，增强数据的多样性，提升模型对不同表情类型的泛化能力。

5. **实验验证与结果评估**

- 模型评估指标：
  - 使用精度、召回率、F1分数等指标来评估模型在微表情和宏表情检测上的性能。
  - 特别地，精度高低可以反映模型对微表情的捕捉能力，召回率则可以反映模型对宏表情检测的全面性。
- 实验验证：
  - 在不同的视频数据集上进行实验，以验证BSN对表情区间检测的适用性。可以选择具有标注的微表情数据集和宏表情数据集进行测试。

6. **实现效果**

- **微表情检测**：通过BSN的高分辨率边界检测和置信度评估，能够捕捉到短暂且细微的表情变化，提高微表情识别的准确性。
- **宏表情识别**：通过对较长时间区间的准确定位，BSN可以有效地识别宏表情，并通过区间长度和变化幅度的分析区分不同的表情类型。

通过将BSN灵活应用于微表情和宏表情的识别任务，可以有效地捕捉并区分这些表情变化的区间，从而实现更加精确的表情分析。这一方法不仅提高了表情识别的准确性，还为复杂的表情动态研究提供了一个强有力的工具。

#### 图卷积网络

> 将人脸作为一个图，以不同的ROIs为节点，采用GCN进行AU检测。通过对AUs对的条件概率设置阈值来构建具有二值的图。

将人脸表示为一个图，以不同的感兴趣区域（Regions of Interest, ROIs）作为节点，并使用图卷积网络（Graph Convolutional Network, GCN）进行面部动作单元（Action Units, AUs）检测，是一个创新且有效的思路。通过这种方法，可以捕捉到面部不同区域之间的相互关系和依赖性，提升AU检测的准确性和鲁棒性。以下是这一方法的详细设计步骤：

1. **将人脸表示为图**

- ROIs作为节点：
  - 将人脸分解为多个感兴趣区域（ROIs），每个ROI对应人脸的一部分，例如眼睛、眉毛、嘴角、鼻子等。这些ROIs代表了与特定AU相关的面部区域。
  - 每个ROI作为图中的一个节点，这些节点可以通过图结构进行建模，从而捕捉到不同面部区域之间的相互作用。
- AU对的条件概率设置阈值构建边：
  - 使用历史数据或预训练模型来计算不同AU对之间的条件概率。例如，某些AUs经常同时激活（如微笑时嘴角和眼睛周围的肌肉），而某些AUs很少同时出现。
  - 设定一个阈值，将条件概率高于阈值的AU对视为存在依赖关系，从而在对应的ROIs节点之间构建边，形成二值图。如果两个AU的条件概率高于设定阈值，则它们对应的两个ROI节点之间连一条边；否则，不连边。

2. **GCN架构设计**

- **输入特征表示**：
  - 对每个ROI提取特征，这些特征可以是来自2D-CNN或其他特征提取器的空间特征。输入特征可以包括图像区域的纹理、颜色、梯度等信息。
  - 将这些特征作为每个节点的初始表示输入到GCN中。
- **图卷积层**：
  - 图卷积操作可以根据图的结构（即节点之间的连接方式）来聚合邻近节点的信息。对于每个ROI节点，GCN会整合其相邻节点（即与之相连的其他ROIs）的特征，从而捕捉到面部区域之间的依赖关系。
  - 通过多层图卷积，模型逐渐扩展感受野，捕捉到更大范围内的面部区域之间的相互作用。
- **非线性激活**：
  - 在每一层图卷积之后，应用非线性激活函数（如ReLU），增加模型的表达能力，帮助捕捉复杂的面部表情模式。
- **全连接层与输出**：
  - 在经过若干图卷积层后，将每个节点（即ROI）的最终表示通过全连接层映射到AU的分类空间，进行AU的激活检测。
  - 最终输出层可以是一个多标签分类器，对每个AU进行独立的二元分类（激活或未激活）。

3. **模型训练**

- **损失函数**：
  - 对每个AU使用二元交叉熵损失（Binary Cross-Entropy Loss），将其视为独立的二元分类问题。
  - 也可以使用联合损失函数，结合多个AUs的检测结果来共同优化模型。
- **训练数据**：
  - 使用带有AUs标注的人脸数据集进行训练，确保模型能够学习到面部区域之间的相关性和不同AUs的表达模式。
- **正则化与增强**：
  - 通过正则化技术（如Dropout、L2正则化）避免过拟合，尤其是在小数据集上的训练。
  - 数据增强策略（如随机裁剪、旋转、镜像等）可以帮助模型更好地泛化到未见过的面部表情。

4. **优势与挑战**

- **优势**：
  - 通过GCN建模面部区域之间的相互关系，能够捕捉到复杂的表情依赖性，提高AU检测的准确性。
  - 这种方法具有更强的泛化能力，因为它考虑了AUs之间的条件依赖关系，而不仅仅是单独地处理每个AU。
- **挑战**：
  - 阈值的设定对图的结构有较大影响，需要通过实验来调节合适的阈值。
  - 在构建图时，可能面临一些计算上的复杂性，特别是在高维特征空间下的图卷积操作。

5. **实验验证与效果**

- **性能评估**：
  - 在标准的AU检测数据集（如BP4D、DISFA等）上进行实验，评估模型的准确率、召回率、F1分数等。
  - 对比传统的AU检测方法，验证GCN在建模面部区域关系和条件依赖方面的优势。
- **效果分析**：
  - 通过模型的可解释性分析（如特征可视化、节点重要性分析），进一步理解GCN是如何利用面部区域之间的关系来提高AU检测性能的。

通过将人脸表示为图并使用GCN进行AU检测，能够充分利用面部区域之间的相互关系，从而提升模型对细微表情和复杂面部动作的识别能力。这种方法不仅在理论上具有创新性，而且在实践中也能带来更好的检测性能。



> 使用GCN进行微表情识别

使用图卷积网络（GCN）进行微表情识别是一种创新的方式，能够有效地捕捉面部区域之间的复杂依赖关系和交互信息。微表情识别的挑战在于微表情短暂、细微且不易察觉，因此，如何从面部不同区域的动态变化中提取有效的特征是关键。下面是如何使用GCN进行微表情识别的详细方法：

1. **图的构建**

- **节点（Nodes）**：
  - 将面部分割为多个感兴趣区域（ROIs），如眼睛、眉毛、嘴角、鼻子等。每个ROI代表一个节点，这些节点携带面部区域的特征信息。
  - ROI的特征可以通过卷积神经网络（CNN）从每帧图像中提取，特征包括局部纹理、梯度、颜色信息等。
- **边（Edges）**：
  - 使用人脸结构信息或统计学方法来确定哪些ROIs之间应该有连接。可以基于欧几里得距离或面部解剖学信息手动设置，也可以通过训练数据中AUs之间的共现关系来自动学习。
  - 使用AUs对之间的条件概率构建二值化的图结构。如果两个ROIs之间的相关性较强（超过某个设定的阈值），则在它们之间建立一条边。

2. **GCN架构设计**

- **输入层**：
  - 输入图由节点特征和图结构（边的连接关系）组成。节点特征是通过CNN提取的ROI特征，图结构是通过前述的方法构建的图。
  - 输入层将这些特征传递给GCN的第一层。
- **图卷积层**：
  - GCN通过在图上进行卷积操作，将每个节点的特征与其相邻节点的特征进行聚合。这种操作能够捕捉到面部不同区域之间的依赖关系。
  - 多层图卷积可以进一步聚合更广泛的面部区域信息，从而识别出微表情所涉及的复杂交互模式。
- **激活函数与池化**：
  - 在每一层图卷积后，使用ReLU等激活函数增加模型的非线性能力。随后，进行池化操作以减少特征维度，同时保留重要的图结构信息。
- **全连接层与输出层**：
  - 在经过若干图卷积层后，得到每个节点的最终表示。将这些表示展平成一个向量，经过全连接层处理，最后输出微表情的分类结果。
  - 输出层可以是一个Softmax层，用于多类别微表情的分类，也可以是多个二元分类器，用于检测每种微表情的激活状态。

3. **模型训练**

- **损失函数**：
  - 对于微表情分类任务，可以使用交叉熵损失（Cross-Entropy Loss）来训练模型。
  - 如果使用二元分类器检测每种微表情的激活状态，则使用二元交叉熵损失（Binary Cross-Entropy Loss）。
- **正则化与优化**：
  - 使用L2正则化和Dropout技术来防止模型过拟合。对于小数据集，正则化手段尤为重要。
  - 使用Adam或SGD等优化器，调节学习率等超参数，确保模型能够有效收敛。

4. **数据增强与预处理**

- **数据增强**：
  - 使用时空数据增强策略，如时间翻转、随机裁剪、旋转、平移等，增加训练数据的多样性，提升模型的泛化能力。
  - 可以通过合成微表情数据或使用数据增强技术扩展数据集，特别是在微表情数据稀缺的情况下。
- **数据预处理**：
  - 对输入视频进行预处理，如帧间差异计算、光流提取等，以增强微表情的动态特征。可以先通过光流或差分帧捕捉微表情的微小运动，再输入GCN进行处理。

5. **模型评估与实验验证**

- **评估指标**：
  - 使用准确率、召回率、F1分数等标准指标评估模型的微表情识别性能。特别是F1分数，对于处理类别不平衡问题（如微表情数据集中某些微表情样本少）的任务非常重要。
  - 可以使用ROC-AUC曲线来评估模型的二元分类性能。
- **实验验证**：
  - 在多个公开的微表情数据集上进行实验，如CASME、SAMM等，验证模型的通用性和性能。
  - 与现有的微表情识别方法进行对比，分析GCN方法的优劣。

6. **优点与挑战**

- **优点**：
  - GCN能够充分利用面部不同区域之间的相互关系，捕捉到微表情产生的微妙变化。
  - 通过图结构建模，GCN能够自然地处理非欧几里得空间数据，如人脸区域的复杂连接方式，提升识别效果。
- **挑战**：
  - 微表情数据的稀缺性仍然是一个难点，尤其是在训练GCN这种复杂模型时。数据增强和转移学习可以在一定程度上缓解这一问题。
  - 构建合理的图结构（即确定哪些ROIs之间应该有边）对模型性能影响较大，需要通过实验调整或使用数据驱动的方法自动学习图结构。

7. **实现效果**

- 在应用GCN进行微表情识别的实验中，预计能够取得比传统方法更优的性能，尤其在捕捉面部不同区域之间的复杂互动上。通过这种方法，可以实现对微表情的更精确识别，有助于在心理学研究、人机交互等领域的应用。



> 把GCN的建模能力应用于宏表情和微表情的定位

将图卷积网络（GCN）的建模能力应用于宏表情和微表情的定位任务，是一种有效的方式来捕捉面部区域之间的复杂关系，并定位出表情发生的具体时间段。以下是如何将GCN应用于宏表情和微表情定位的详细步骤：

1. **定义问题**

- **宏表情定位**：确定宏表情在视频中出现的起始和结束时间段。宏表情通常持续较长时间，并伴随较明显的面部肌肉活动。
- **微表情定位**：检测微表情在视频中的具体位置。微表情持续时间短，变化迅速，且不易察觉。

2. **图的构建**

- **节点（Nodes）**：
  - 将面部分解为多个感兴趣区域（ROIs），如眼睛、眉毛、嘴角、鼻子等。每个ROI代表一个节点，这些节点携带面部区域的特征信息。
  - 节点的特征可以通过2D-CNN或3D-CNN从视频帧中提取，特征包括空间特征（如纹理、梯度）和时间特征（如帧间差异、光流）。
- **边（Edges）**：
  - 使用面部结构信息、AUs共现关系或统计学方法确定ROIs之间的连接方式。构建的图表示了面部区域之间的依赖关系和相互影响。
  - 边的权重可以根据AUs之间的条件概率或基于欧几里得距离来设置，反映ROIs之间的相互关系。

3. **GCN用于表情特征提取**

- **输入层**：
  - 将每个ROI节点的初始特征输入到GCN中，这些特征可以是从CNN提取的空间和时间特征。
- **图卷积层**：
  - 通过图卷积操作，将每个节点的特征与其相邻节点的特征进行聚合，从而捕捉到面部区域之间的交互关系。
  - 多层图卷积可以扩展感受野，使模型能够捕捉到面部区域之间更复杂的依赖关系。
- **时间信息整合**：
  - 对于宏表情定位，时间信息尤为关键。可以通过结合RNN（如LSTM）来处理GCN输出的时间序列特征，或通过3D-CNN直接在视频帧序列上提取时空特征。

4. **表情定位模块**

- **候选区间生成**：
  - 在视频序列中，通过滑动窗口方法生成多个候选时间区间。这些区间将作为候选的表情发生区间。
  - 每个区间的特征可以通过在时间维度上聚合对应的GCN节点输出特征来获得。
- **区间分类与回归**：
  - 使用区间分类模块对每个候选区间进行分类，判断其是否包含宏表情或微表情。
  - 使用回归模块进一步细化区间的边界（起始时间和结束时间），确保准确定位表情发生的时间段。

5. **损失函数设计**

- **分类损失**：
  - 使用交叉熵损失（Cross-Entropy Loss）来优化候选区间的表情分类性能。
  - 可以使用多任务学习，将宏表情和微表情的分类任务结合起来进行联合训练。
- **回归损失**：
  - 使用平滑L1损失（Smooth L1 Loss）或L2损失来优化区间边界回归模型，使得模型能够更精确地定位表情的开始和结束时间。

6. **模型训练与优化**

- **训练数据**：
  - 使用带有宏表情和微表情标注的标准视频数据集进行训练，如CASME、SAMM等。
  - 数据增强可以通过改变视频的播放速度、镜像翻转等手段增加数据多样性，提升模型的泛化能力。
- **正则化与超参数调节**：
  - 使用Dropout、L2正则化等手段防止模型过拟合，尤其是在处理小数据集时。
  - 调整学习率、批次大小等超参数，确保模型在训练过程中能够有效收敛。

7. **模型评估与实验验证**

- **评估指标**：
  - 使用准确率、召回率、F1分数等指标评估模型在宏表情和微表情定位上的性能。特别关注定位精度和召回率。
  - 通过平均精度（mAP）或IoU（Intersection over Union）来评估区间定位的准确性。
- **实验验证**：
  - 在不同的视频数据集上进行实验，验证GCN在捕捉面部区域关系和表情定位上的效果。
  - 与其他方法进行对比，分析GCN方法在表情定位任务中的优势。

8. **优势与挑战**

- **优势**：
  - GCN能够建模面部不同区域之间的复杂依赖关系，这对于准确定位微表情和宏表情非常有利。
  - GCN结合时间特征，可以在时空上捕捉表情的发生与演变，提高定位的精度。
- **挑战**：
  - 微表情的短暂性和不明显性使得定位变得困难，可能需要更多的数据增强和精细化的模型设计来提高定位效果。
  - 如何合理构建图结构和边的权重对模型性能影响较大，可能需要通过实验反复调试。

9. **实际应用**

- 通过在情绪识别、心理学研究、人机交互等领域的应用，GCN在宏表情和微表情定位上的能力可以进一步验证并推广。特别是在实时系统中，准确快速的表情定位能够带来显著的性能提升。

通过将GCN应用于宏表情和微表情的定位任务，模型能够有效捕捉面部区域之间的复杂交互关系，提升表情定位的准确性和鲁棒性。这一方法为表情分析领域提供了新的思路和技术手段。



### 方法

#### 特征提取

> 使用Retinaface定位视频中第一帧的人脸边界框，并将后续帧与第一帧对齐

在视频处理中，定位第一帧中的人脸边界框并将后续帧与第一帧对齐是非常重要的操作，尤其在表情识别、身份验证等任务中。以下是如何使用RetinaFace定位视频中第一帧的人脸边界框，并将后续帧与第一帧对齐的详细步骤：

1. **RetinaFace模型简介**

- **RetinaFace** 是一种基于深度学习的人脸检测器，能够精准地定位人脸关键点并生成边界框。它不仅能够检测出人脸的位置，还可以输出五个关键点（如眼睛、鼻子、嘴角等），用于进一步的面部对齐操作。

2. **第一帧人脸边界框定位**

- **加载RetinaFace模型**：
  - 使用预训练的RetinaFace模型进行人脸检测。可以选择MXNet、PyTorch等实现版本，根据需求加载相应的模型权重。
- **检测第一帧中的人脸**：
  - 读取视频的第一帧，并使用RetinaFace模型对该帧进行人脸检测。
  - 输出人脸的边界框（Bounding Box）和五个关键点的位置（如眼角、鼻尖、嘴角）。
- **保存边界框和关键点**：
  - 记录第一帧中检测到的人脸边界框的坐标（左上角和右下角的坐标）。
  - 同时记录人脸关键点的位置，这些关键点将在后续帧对齐过程中使用。

3. **对齐后续帧**

- **读取视频后续帧**：
  - 逐帧读取视频的后续帧，准备对齐操作。
- **人脸关键点检测**：
  - 使用RetinaFace模型对每一帧进行人脸检测，获取该帧中人脸的关键点。
  - 如果RetinaFace模型未能在某些帧中检测到人脸，可以尝试使用其他帧或通过插值方法估计关键点位置。
- **计算仿射变换矩阵**：
  - 使用第一帧中的关键点位置作为基准，将每一帧检测到的关键点与第一帧的关键点进行匹配。
  - 通过计算仿射变换矩阵（Affine Transformation Matrix），将后续帧的关键点变换到与第一帧关键点对齐的位置。
- **应用变换**：
  - 使用计算得到的仿射变换矩阵对后续帧进行变换。该变换不仅对齐了关键点，还调整了整个人脸的边界框位置和大小。
  - 可以选择直接对原始帧应用变换，或仅在后续处理时对人脸区域应用变换。

4. **保存对齐结果**

- **保存对齐后的视频**：
  - 将对齐后的每一帧重新组合为一个新的视频文件，确保每一帧中的人脸位置保持一致。
  - 对齐后的帧可以用于后续的表情分析、身份识别等任务。
- **进一步处理**：
  - 对齐后的视频可以用于特征提取、表情识别或进一步的时序分析。因为所有帧中的人脸位置和大小已经对齐，这将大大简化后续的处理工作。

5. **代码实现示例**

以下是一个简化的Python示例代码，展示如何使用RetinaFace进行上述操作：

```python
import cv2
import numpy as np
from retinaface import RetinaFace

def align_face(video_path, output_path):
    # 打开视频文件
    cap = cv2.VideoCapture(video_path)
    ret, first_frame = cap.read()
    
    if not ret:
        print("无法读取视频")
        return
    
    # 使用RetinaFace检测第一帧中的人脸
    faces = RetinaFace.detect_faces(first_frame)
    if faces:
        # 假设检测到的第一个人脸是目标
        face_info = faces[list(faces.keys())[0]]
        bbox = face_info["facial_area"]
        landmarks = face_info["landmarks"]
        
        # 记录第一帧的人脸边界框和关键点
        ref_landmarks = np.array([
            landmarks["left_eye"],
            landmarks["right_eye"],
            landmarks["nose"],
            landmarks["mouth_left"],
            landmarks["mouth_right"]
        ])
        
        # 创建视频写入对象
        fourcc = cv2.VideoWriter_fourcc(*'XVID')
        out = cv2.VideoWriter(output_path, fourcc, 20.0, (first_frame.shape[1], first_frame.shape[0]))
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # 在当前帧中检测人脸
            faces = RetinaFace.detect_faces(frame)
            if faces:
                face_info = faces[list(faces.keys())[0]]
                landmarks = face_info["landmarks"]
                
                curr_landmarks = np.array([
                    landmarks["left_eye"],
                    landmarks["right_eye"],
                    landmarks["nose"],
                    landmarks["mouth_left"],
                    landmarks["mouth_right"]
                ])
                
                # 计算仿射变换矩阵
                matrix = cv2.estimateAffinePartial2D(curr_landmarks, ref_landmarks)[0]
                
                # 对当前帧进行仿射变换
                aligned_frame = cv2.warpAffine(frame, matrix, (frame.shape[1], frame.shape[0]))
                
                # 写入对齐后的帧
                out.write(aligned_frame)
            else:
                out.write(frame)
        
        cap.release()
        out.release()

# 示例调用
align_face('input_video.mp4', 'output_video.avi')
```

6. **优势与挑战**

- **优势**：
  - 使用RetinaFace进行人脸检测具有高精度和鲁棒性，能够有效处理不同角度、光照和表情变化下的人脸。
  - 对齐操作确保了视频中所有帧中的人脸位置一致，有助于后续的表情分析和识别任务。
- **挑战**：
  - 如果视频帧中存在较大的人脸旋转或遮挡，可能会影响关键点检测的准确性，进而影响对齐效果。
  - 实时处理较长的视频时，计算资源需求较高，可能需要优化代码或使用GPU加速。

通过上述方法，能够精准地定位和对齐视频中的人脸，从而为后续的分析和处理打下坚实的基础。



> 采用TV-L1算法计算光流特征

使用TV-L1算法计算光流特征是视频处理中的一种经典方法，特别适用于检测和跟踪视频中物体的运动。以下是一个详细的步骤，说明如何使用TV-L1算法计算光流特征。

1. **安装必要的库**

确保你已经安装了OpenCV库。如果没有，可以使用以下命令安装：

```bash
pip install opencv-python opencv-contrib-python
```

2. **初始化TV-L1光流计算器**

首先，导入OpenCV并初始化TV-L1光流计算器。

```python
import cv2

# 创建TV-L1光流计算器
optical_flow = cv2.optflow.DualTVL1OpticalFlow_create()
```

3. **读取视频并初始化第一帧**

接下来，我们读取视频并获取第一帧，将其转换为灰度图像，准备计算光流。

```python
# 读取视频
cap = cv2.VideoCapture('input_video.mp4')
ret, prev_frame = cap.read()

if not ret:
    print("无法读取视频")
    exit()

# 将第一帧转换为灰度图像
prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
```

4. **逐帧计算光流**

我们将在每一帧与前一帧之间计算光流。

```python
while True:
    # 读取当前帧
    ret, curr_frame = cap.read()
    if not ret:
        break

    # 将当前帧转换为灰度图像
    curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)

    # 计算光流
    flow = optical_flow.calc(prev_gray, curr_gray, None)

    # 可视化光流（使用颜色编码）
    hsv = np.zeros_like(prev_frame)
    hsv[..., 1] = 255  # 饱和度设为最大值

    # 计算光流的角度和幅度
    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])
    hsv[..., 0] = ang * 180 / np.pi / 2  # 将角度映射到色调
    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)  # 将幅度映射到亮度

    # 将HSV转换为BGR格式以便显示
    flow_rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)

    # 显示光流结果
    cv2.imshow('Optical Flow', flow_rgb)

    # 更新前一帧
    prev_gray = curr_gray

    # 按'q'键退出
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

5. **代码解释**

- **光流计算**：
  - `optical_flow.calc(prev_gray, curr_gray, None)`：这是计算光流的核心函数。它将前一帧与当前帧的灰度图作为输入，并输出一个光流场，其中每个像素点包含x和y方向上的运动分量。
- **光流可视化**：
  - 使用HSV颜色空间将光流进行可视化。角度（方向）映射到色调（H），幅度（运动大小）映射到亮度（V）。通过这种方式，能够直观地观察到视频中各个像素点的运动情况。
- **视频处理循环**：
  - 逐帧读取视频并计算光流。处理完当前帧后，更新前一帧为当前帧，以便在下一循环中使用。

6. **优化与应用**

- **性能优化**：
  - 对于长视频或高分辨率视频，光流计算可能较为耗时。可以通过降低帧率或分辨率来加速处理。
  - 可以将计算好的光流特征保存为文件，以便在后续处理中重复使用。
- **应用场景**：
  - **动作识别**：光流特征可以用于识别视频中的动作和行为模式。
  - **目标跟踪**：通过分析连续帧之间的光流，可以跟踪视频中物体的运动轨迹。
  - **视频稳定**：利用光流信息，可以校正视频中的抖动，生成更稳定的视频输出。

7. **保存光流特征**

如果需要将计算的光流特征保存为文件，可使用以下代码：

```python
frame_index = 0
while True:
    ret, curr_frame = cap.read()
    if not ret:
        break

    curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)
    flow = optical_flow.calc(prev_gray, curr_gray, None)

    # 保存光流结果为文件
    np.save(f'flow_{frame_index}.npy', flow)

    prev_gray = curr_gray
    frame_index += 1
```

这样每一帧的光流特征将被保存为`.npy`文件，供后续分析使用。

通过以上步骤，能够使用TV-L1算法计算视频的光流特征，并进行可视化或保存供后续处理。光流特征在计算机视觉和视频分析中具有广泛的应用。



> 为了细化特征，我们按照[9]的方法，选择12个ROIs，并使用SAN检测到的面部地标进行裁剪
>
> [9]Yong-Jin Liu, Jin-Kai Zhang, Wen-Jing Yan, Su-Jing Wang, Guoying Zhao, and Xiaolan Fu, “A main directional mean optical flow feature for spontaneous micro-expression recognition,” IEEE Transactions on Affective Computing, vol. 7, no. 4, pp. 299–310, 2015.

在按照[9]的方法进行特征细化时，通过选择12个感兴趣区域（ROIs）并使用SAN（姿态自适应网络）检测到的面部地标进行裁剪，可以有效捕捉微表情的细微变化。下面是具体的步骤和思路：

1. **选择12个ROIs**

- 根据面部表情变化的特点，12个ROIs通常包括以下区域：
  - **左右眉毛**（2个ROIs）
  - **左右眼睛**（2个ROIs）
  - **左右脸颊**（上下各2个，共4个ROIs）
  - **鼻子**（1个ROI）
  - **嘴巴区域**（包括上下唇，共2个ROIs）
  - **下巴**（1个ROI）

2. **使用SAN检测面部地标**

- 使用SAN（姿态自适应网络）来检测面部的68个地标点。这些地标点将用于定义每个ROI的边界。例如，眼睛区域的ROI可以由对应的眉毛和眼睑的地标点确定。
- SAN的优点在于它可以在不同姿态下准确检测面部特征，确保ROI的裁剪更加精确。

3. **裁剪ROIs**

- 根据检测到的面部地标，裁剪出12个ROIs。具体方法是使用这些地标点来定义每个ROI的边界框。裁剪后，每个ROI只包含面部的一个特定区域，如眼睛或嘴巴。
- 裁剪的具体过程可以参考以下Python代码：

```python
import cv2
import numpy as np

def crop_roi(image, landmarks, points):
    # 提取ROI的边界框
    x_min = np.min(landmarks[points, 0])
    x_max = np.max(landmarks[points, 0])
    y_min = np.min(landmarks[points, 1])
    y_max = np.max(landmarks[points, 1])

    # 裁剪ROI
    roi = image[y_min:y_max, x_min:x_max]
    return roi

# 例如：定义眼睛区域的地标点索引
left_eye_points = [36, 37, 38, 39, 40, 41]  # 左眼的地标点
right_eye_points = [42, 43, 44, 45, 46, 47] # 右眼的地标点

# 使用SAN检测面部地标（假设landmarks是检测到的68个点）
# landmarks = detect_landmarks(image)

# 裁剪左眼ROI
left_eye_roi = crop_roi(image, landmarks, left_eye_points)

# 裁剪右眼ROI
right_eye_roi = crop_roi(image, landmarks, right_eye_points)

# 其他ROI可以类似裁剪
```

4. **细化特征**

- 在每个ROI内，计算光流特征或其他相关的表情特征。Liu等人提出的方法主要是基于光流的特征，即主方向平均光流（Main Directional Mean Optical Flow, MDMO）。
- 对于每个ROI，使用TV-L1算法计算光流，然后提取该区域的主方向和平均光流大小作为特征，即MDMO特征。

5. **MDMO特征计算示例**

- 计算光流之后，提取每个ROI中的光流方向，并计算这些方向的主方向及其平均光流大小。
- 将每个ROI的MDMO特征组合起来，形成最终的特征向量。这个特征向量可以作为输入，传递给分类器进行微表情识别。

```python
def calculate_mdmo(flow):
    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])
    # 计算主方向平均光流
    mean_direction = np.mean(ang)
    mean_magnitude = np.mean(mag)
    return mean_direction, mean_magnitude

# 假设我们已计算了左眼ROI的光流
left_eye_flow = optical_flow.calc(prev_left_eye, curr_left_eye, None)
left_eye_mdmo = calculate_mdmo(left_eye_flow)
```

6. **构建特征向量**

- 将每个ROI的特征组合起来，形成最终的特征向量。这个特征向量将用于后续的微表情识别模型训练和测试。

通过这种方法，能够更加精细地捕捉面部不同区域的运动模式，提升微表情识别的效果。



> 计算细粒度MDMO特征作为模型输入

为了计算细粒度的主方向平均光流特征（MDMO），并将其作为模型输入，我们可以遵循以下步骤：

1. **预处理视频帧并提取ROIs**

- 使用SAN检测面部地标，将面部划分为12个感兴趣区域（ROIs）。
- 对每一帧进行ROI裁剪，确保仅包含面部的关键区域。

2. **计算光流**

- 使用TV-L1光流算法计算连续帧之间的光流。这会生成一个光流场，其中每个像素都有水平（x）和垂直（y）方向的运动分量。
- 对每个ROI单独计算光流。

```python
import cv2

def calculate_optical_flow(prev_img, curr_img):
    optical_flow = cv2.optflow.DualTVL1OpticalFlow_create()
    flow = optical_flow.calc(prev_img, curr_img, None)
    return flow
```

3. **提取MDMO特征**

- 对每个ROI，计算主方向平均光流（MDMO）特征。MDMO特征包括光流的方向和幅度。

```python
import numpy as np

def calculate_mdmo(flow):
    # 计算光流的幅度和角度
    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])

    # 计算主方向平均光流
    mean_direction = np.mean(ang)  # 平均方向
    mean_magnitude = np.mean(mag)  # 平均幅度

    return mean_direction, mean_magnitude
```

4. **计算细粒度MDMO特征**

- 对每个ROI的光流计算MDMO特征，并将这些特征结合在一起形成特征向量。

```python
def extract_features_from_rois(rois, prev_frame, curr_frame):
    features = []
    for roi in rois:
        # 裁剪ROI
        prev_roi = prev_frame[roi[1]:roi[3], roi[0]:roi[2]]
        curr_roi = curr_frame[roi[1]:roi[3], roi[0]:roi[2]]

        # 计算光流
        flow = calculate_optical_flow(prev_roi, curr_roi)

        # 计算MDMO特征
        mdmo = calculate_mdmo(flow)
        features.extend(mdmo)

    return features
```

- 这里，`rois`是一个列表，包含每个ROI的坐标（x_min, y_min, x_max, y_max）。

5. **将MDMO特征作为模型输入**

- 将每一帧的特征组合成一个大的特征矩阵，其中每一行代表一个帧的特征，列是不同ROIs的MDMO特征。
- 这些特征可以直接输入到分类器（如SVM、随机森林或深度学习模型）中，用于微表情识别。

```python
# 假设我们有一系列的帧
features_list = []
for i in range(1, len(frames)):
    features = extract_features_from_rois(rois, frames[i-1], frames[i])
    features_list.append(features)

# 将特征列表转换为numpy数组
features_array = np.array(features_list)

# 输入到模型
model.fit(features_array, labels)
```

6. **模型训练与测试**

- 使用上述提取的MDMO特征进行模型训练。可以选择合适的分类器或回归模型，根据任务要求（如微表情分类）进行训练。
- 使用标准的评估指标（如准确率、F1-score）对模型进行评估。

总结

通过计算细粒度的MDMO特征，你可以从面部的不同区域提取出有意义的运动信息，并将其作为模型输入，以提高微表情识别的准确性。这种方法尤其适用于处理小数据集和需要细致分析的任务场景。



### 实验

> 根据MEGC2021[27]的协议，我们在实验中采用了Leave-One-Subject-Out (LOSO)交叉验证策略。

**1.LOSO协议是什么？**

LOSO协议（Leave-One-Subject-Out）是一种交叉验证策略，主要用于小样本数据集，尤其是在个体差异较大的数据集上，如医学影像分析或脑电图（EEG）等生物医学信号处理中。其核心思想是将数据集中每个个体（或受试者）的数据依次作为验证集，剩余个体的数据作为训练集进行模型训练和验证。

LOSO协议的步骤：

1.划分数据集：对于一个包含N个个体的数据集，先从中选择一个个体的数据作为验证集。

2.训练模型：使用剩余N-1个个体的数据训练模型。

3.验证模型：将训练好的模型在选定的验证集上进行验证，记录其性能指标。

4.重复步骤：重复上述步骤N次，每次选择不同的个体作为验证集，其余作为训练集 。

5.计算平均性能：最终，将所有N次验证的性能指标取平均值，作为模型的总体性能。

LOSO的优点：

* 个体间差异考虑：可以有效考虑个体间的差异，特别适用于个体差异较大的数据集。
* 充分利用数据：相比于传统的*k折交叉验证*，LOSO使得每个个体的数据都可以被用作验证集和训练集。

LOSO的缺点：

* 计算开销大：对于大数据集或包含大量个体的数据集，进行多次训练和验证的计算开销较大。
* 模型不稳定：如果某些个体的数据与其他个体的数据有很大差异，可能会导致模型在某些折中的表现不佳。

这种方法常用于需要精细调优的领域，如医学、心理学和生物医学信号处理等。

*k折交叉验证（k-fold cross-validation）*是一种常用的模型验证方法，用于评估机器学习模型的性能和泛化能力。它通过将数据集分成k个相同大小的子集（或称为“折”），来多次训练和验证模型，以减少由于随机分割数据集导致的评估结果不稳定性。

k折交叉验证的步骤：

1.划分数据集：将整个数据集随机分成k个不重叠的子集，通常k的值为5或10。

2.迭代训练和验证：

* 在第i次迭代中，选择第i个子集作为验证集，其他k-1个子集作为训练集。
* 使用训练集训练模型。
* 使用验证集评估模型的性能，并记录性能指标（如准确率、精确率、召回率等）。

3.重复k次：重复上述步骤，共进行k次迭代，每次选择不同的子集作为验证集。

4.计算平均性能：将k次迭代的性能指标取平均值，作为模型的总体性能。

k折交叉验证的优点：

* 减少偏差：通过多次训练和验证，k折交叉验证减少了由于数据随机分割导致的结果偏差。
* 充分利用数据：与简单的训练/验证数据集 划分方法相比，k折交叉验证能够更有效地利用数据集中的每一个样本。
* 适用于小数据集：在数据集较小的情况下，这种方法尤其有用，因为它能够最大限度地利用有限的数据。

k折交叉验证的缺点：

* 计算开销：k折交叉验证需要进行k次训练和验证，计算量较大，特别是对于大数据集或复杂模型。
* 不适用于时间序列数据：对于时间序列数据，这种方法可能不合适，因为它打乱了数据的实践顺序，无法保持时间依赖性。

特殊形式：

* 留一法（LOOCV）：k折交叉验证的一种极端情况，当k等于数据集的样本数时，这种方法称为留一法（Leave-One-Out Cross-Validation,LOOCV）。每次只留一个样本作为验证集，其余样本作为训练集。

k折交叉验证是机器学习中广泛应用的一种验证方法，它能够提供关于模型泛化能力的较为可靠的估计。

来源：ChatGPT

## 论文重点分析

1.为什么使用数据集时没有使用RGB图像？使用的是npz文件？是否可以用其他数据集替代？

2.如何改进？

3.为什么数据集的编号是从015开始，而不是从0开始？之前的数据不需要吗？



## 阅读感想

1.训练模型时，不是直接使用的图片和视频，而是使用npz数组。

2.项目中交代了面部关键点和兴趣区域的提取的具体方法

## 代码分析

1.使用两个数据集CAS(ME)^2和SAMM-LV，代码中是分别训练，但是在在最终结果统计中，综合统计了两个数据集的训练效果，~~在代码中有`cross.csv`这样类似的文件~~（在项目中发现cross是一个数据集），猜测是否使用两个数据集进行交叉训练和验证。



2.代码读入数据集时没有直接读取图片和视频，使用的是处理好的数据集，形式上是分块的`npz`文件，猜测是最初使用图片和视频以及AU信息进行处理之后形成的`npz`文件，虽然在项目中存在相关的面部信息处理的代码文件，但不能确定是否是从源图片和视频处进行处理。如果从源图片和视频进行处理，但是没从代码文件中找到相关读取图片和视频的路径。

3.为什么待处理的数据集命名不从0开始，比如casme从015开始，samm从007开始？猜测只处理视频，在项目给出的图片中的数据预处理部分，给出的是输入视频，但是还未进行验证。

4.代码的输出结果没有输出图片一类，可能需要自己写一部分代码，将输出的结果使用曲线图或折线图进行表示。

5.输出结果中还有`pth`一类的文件，但是是分块的，一个数据集有多个`pth`文件。一般认为跑一次代码后获得的模型，下一次跑代码时可以使用上一次的模型，但是在代码中没找到类似导入之前的模型的代码，不过找到数据预处理的一个函数，这个函数中的读入的路径是项目中不存在的路径。所以是否需要自己写相关的读入上次训练的模型的代码？

6.项目中提供的csv文件与数据集中的xlsx文件中的au区域有冲突，xlsx文件中有的au值为null，不知xlsx中的数据是否值得可信？暂时先用项目中的csv文件。

7.项目中有原始视频和图像处理的代码，第一次运行时出现了各种意外的错误。





