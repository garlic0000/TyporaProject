# AU-aware graph convolutional network for Macro- and Micro-expression spotting

基于AU感知的宏表情和微表情识别的图卷积网络  

DOI:10.48550/arXiv.2303.09114  

## 论文详细分析

### 摘要

摘要逻辑部分如下：

1.叙述视频中微表情识别的特点和难点

2.叙述本文的改进方法

> 建模面部感兴趣区域（ROI）之间的关系来提取更精细的空间特征

参考:

[GCN—图卷积神经网络理解_gcn提取空间特征-CSDN博客](https://blog.csdn.net/wsp_1138886114/article/details/100709692)

[机器学习术语表 | Freeopen](https://freeopen.github.io/glossary/)

这里的空间特征是机器学习中的概念。

对于图片而言，中心点像素及其邻域像素构成空间，这些像素的值和关系构成特征。


> 提出了一个基于图卷积的网络，成为动作单元感知图卷积网络（AUW-GCN）

参考：

[GCN—图卷积神经网络理解_gcn提取空间特征-CSDN博客](https://blog.csdn.net/wsp_1138886114/article/details/100709692)

卷积一般指离散卷积，其本质是加权求和。

通过计算中心像素点以及相邻像素点的加权和来构成特征图谱以实现空间特征的提取。

传统的CNN（卷积神经网络）处理的图像或者视频的像素点是排列的很整齐的矩阵，但实际上有许多数据及其关系不是规整的矩阵，其网络结构为拓扑图，而GCN就是用于处理拓扑结构，用于弥补CNN无法处理拓扑图的缺陷。

图卷积是将一个节点周围的邻居按照不同的权重叠加起来，每个节点的周围的的邻居数不固定。但是一般卷积的节点的邻居数是固定的。

参考：

[图神经网络之图卷积网络——GCN_图卷积的神经网络-CSDN博客](https://blog.csdn.net/zbp_12138/article/details/110246797)

有一些公式不太理解。

图卷积相比图像卷积更灵活。

> 为了注入先验信息和解决小数据集的问题，将AU相关的统计信息编码到网络中

参考：

[如何将先验知识注入推荐模型-CSDN博客](https://blog.csdn.net/Kaiyuan_sjtu/article/details/121987636)

是不是预处理模型？

参考：

[【干货指南】机器学习必须需要大量数据？小数据集也能有大价值！ (qq.com)](https://mp.weixin.qq.com/s/xGnDcRtKKt4FyVRAMPSqYA)

参考：

[【论文阅读】AU检测|《Deep Adaptive Attention for Joint Facial Action Unit Detection and Face Alignment》_au 关键点检测-CSDN博客](https://blog.csdn.net/qq_43521527/article/details/113440970)

不太能理解。

> 在CAS(ME)2和SAMM-LV两个基准数据集上取得了新的SOTA性能

参考：

[论文里SOTA性能的理解-CSDN博客](https://blog.csdn.net/wuyeyoulan23/article/details/123756127)

大概是最好、最先进的意思。

### 引言

引言部分的逻辑如下：

1.叙述面部表情与情绪关系，并列举相关论文

2.叙述微表情与宏表情两者的概念与区别，并列举相关论文

3.叙述微表情的特点及其应用领域，并列举相关论文

4.叙述微表情研究领域的通用难题，并列举相关论文

5.叙述微表情研究方面取得的进展，引入相关论文进行举例

6.叙述本文在基于这些进展上取得的优化

7.叙述本文的工作内容

> 微表情研究的一个重要部分是微表情的定位（发现），包括在未剪辑的长视频中定位表情间隔

> 以往的研究主要使用原始特征作为输入，包括RGB图像和光流图。

> 使用RGB样本对分别用作ME和MaE的模型输入

> RGB图像在表征面部运动，特别是微表情的面部运动方面是不够的。

> 采用GCN作为特征嵌入主干来表征人脸不同区域之间的关系

> 修改ABPN用于时间特征交互，以更好地利用视频中的上下文信息

> 为了注入面部表情的先验信息，缓解小数据集带来的过拟合问题，我们提出将AU统计信息编码到GCN中，以更好地捕捉面部表情的运动模式，实现更精细的特征表示

### 相关工作

#### 微表情识别

微表情识别部分的逻辑：

1.叙述微表情识别使用的人工方法，列举相关论文进行说明

2.叙述微表情识别使用的深度学习方法，列举相关论文进行说明

3.叙述本文使用的相关的深度学习方法

> 研究人员会选择一个标准的架构，包括cnn和rnn，并将任务视为一个典型的监督问题。

> 分别使用2D-CNNs和1D-CNNs提取空间特征和时间特征，并使用回归网络来细化区间

> 采用3D-CNNs同时提取时空特征

> 将BSN灵活地用于识别微表情和宏表情的间隔

#### 图卷积网络

图卷积网络部分的逻辑：

1.叙述图卷积网络进行AUs检测和微表情识别，并列举相关论文进行说明

2.叙述本文利用图卷网络的建模能力进行微表情和宏表情的定位

> 将人脸作为一个图，以不同的ROIs为节点，采用GCN进行AU检测。通过对AUs对的条件概率设置阈值来构建具有二值的图。

> 使用GCN进行微表情识别

> 把GCN的建模能力应用于宏表情和微表情的定位

### 方法

#### 特征提取

特征提取部分的逻辑：

1.简要叙述特征提取过程包括人脸对准、关键点检测、光流计算和MDMO特征提取

2.详细说明使用Retinaface定位视频进行人脸对准，并列出相关论文

3.详细说明使用TV-L1算法计算光流特征，并列出相关论文

4.详细说明使用SAN检测的面部关键点和选择的12个兴趣区域进行面部裁剪，并列出相关论文

5.详细说明计算MDMO特征作为模型输入，并列出相关论文

> 使用Retinaface定位视频中第一帧的人脸边界框，并将后续帧与第一帧对齐

1.因为使用了滑动窗口技术将长视频分为多个片段，与第一帧对齐，到底是与整个视频的第一帧还是与每个片段的第一帧？

> 在使用滑动窗口技术将长视频分割成片段时，人脸对准的方式通常有两种选择：
>
> 1. **与每个片段的第一帧对准**：
>    - 这种方法对每个片段进行独立的对齐操作，即每个片段中的所有帧都与该片段的第一帧进行对齐。这样可以减少局部帧之间的对齐误差，使每个片段的对齐更精准，尤其当片段内的头部运动较小时效果更好。
> 2. **与整个长视频的第一帧对准**：
>    - 在这种方式下，所有片段的帧都与整个长视频的第一帧进行对齐。这个方法的优势在于，整个视频中的人脸位置保持一致，便于后续特征提取和分析，但如果视频中的人脸出现大幅度的运动，可能会引入更多对齐误差。
>
> ### 选择依据：
>
> - 如果片段内的人脸运动较小，可以选择**与片段第一帧对齐**，以确保每个片段的对齐精度。
> - 如果希望整个视频中人脸位置的一致性，可以选择**与长视频的第一帧对齐**。
>
> 具体选择取决于视频中人脸的运动幅度和模型对对齐精度的需求。

> 采用TV-L1算法计算光流特征

使用TV-L1算法计算光流特征是视频处理中的一种经典方法，特别适用于检测和跟踪视频中物体的运动。以下是一个详细的步骤，说明如何使用TV-L1算法计算光流特征。

1. **安装必要的库**

确保你已经安装了OpenCV库。如果没有，可以使用以下命令安装：

```bash
pip install opencv-python opencv-contrib-python
```

2. **初始化TV-L1光流计算器**

首先，导入OpenCV并初始化TV-L1光流计算器。

```python
import cv2

# 创建TV-L1光流计算器
optical_flow = cv2.optflow.DualTVL1OpticalFlow_create()
```

3. **读取视频并初始化第一帧**

接下来，我们读取视频并获取第一帧，将其转换为灰度图像，准备计算光流。

```python
# 读取视频
cap = cv2.VideoCapture('input_video.mp4')
ret, prev_frame = cap.read()

if not ret:
    print("无法读取视频")
    exit()

# 将第一帧转换为灰度图像
prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
```

4. **逐帧计算光流**

我们将在每一帧与前一帧之间计算光流。

```python
while True:
    # 读取当前帧
    ret, curr_frame = cap.read()
    if not ret:
        break

    # 将当前帧转换为灰度图像
    curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)

    # 计算光流
    flow = optical_flow.calc(prev_gray, curr_gray, None)

    # 可视化光流（使用颜色编码）
    hsv = np.zeros_like(prev_frame)
    hsv[..., 1] = 255  # 饱和度设为最大值

    # 计算光流的角度和幅度
    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])
    hsv[..., 0] = ang * 180 / np.pi / 2  # 将角度映射到色调
    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)  # 将幅度映射到亮度

    # 将HSV转换为BGR格式以便显示
    flow_rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)

    # 显示光流结果
    cv2.imshow('Optical Flow', flow_rgb)

    # 更新前一帧
    prev_gray = curr_gray

    # 按'q'键退出
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

5. **代码解释**

- **光流计算**：
  - `optical_flow.calc(prev_gray, curr_gray, None)`：这是计算光流的核心函数。它将前一帧与当前帧的灰度图作为输入，并输出一个光流场，其中每个像素点包含x和y方向上的运动分量。
- **光流可视化**：
  - 使用HSV颜色空间将光流进行可视化。角度（方向）映射到色调（H），幅度（运动大小）映射到亮度（V）。通过这种方式，能够直观地观察到视频中各个像素点的运动情况。
- **视频处理循环**：
  - 逐帧读取视频并计算光流。处理完当前帧后，更新前一帧为当前帧，以便在下一循环中使用。

6. **优化与应用**

- **性能优化**：
  - 对于长视频或高分辨率视频，光流计算可能较为耗时。可以通过降低帧率或分辨率来加速处理。
  - 可以将计算好的光流特征保存为文件，以便在后续处理中重复使用。
- **应用场景**：
  - **动作识别**：光流特征可以用于识别视频中的动作和行为模式。
  - **目标跟踪**：通过分析连续帧之间的光流，可以跟踪视频中物体的运动轨迹。
  - **视频稳定**：利用光流信息，可以校正视频中的抖动，生成更稳定的视频输出。

7. **保存光流特征**

如果需要将计算的光流特征保存为文件，可使用以下代码：

```python
frame_index = 0
while True:
    ret, curr_frame = cap.read()
    if not ret:
        break

    curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)
    flow = optical_flow.calc(prev_gray, curr_gray, None)

    # 保存光流结果为文件
    np.save(f'flow_{frame_index}.npy', flow)

    prev_gray = curr_gray
    frame_index += 1
```

这样每一帧的光流特征将被保存为`.npy`文件，供后续分析使用。

通过以上步骤，能够使用TV-L1算法计算视频的光流特征，并进行可视化或保存供后续处理。光流特征在计算机视觉和视频分析中具有广泛的应用。



> 为了细化特征，我们按照[9]的方法，选择12个ROIs，并使用SAN检测到的面部地标进行裁剪
>
> [9]Yong-Jin Liu, Jin-Kai Zhang, Wen-Jing Yan, Su-Jing Wang, Guoying Zhao, and Xiaolan Fu, “A main directional mean optical flow feature for spontaneous micro-expression recognition,” IEEE Transactions on Affective Computing, vol. 7, no. 4, pp. 299–310, 2015.

在按照[9]的方法进行特征细化时，通过选择12个感兴趣区域（ROIs）并使用SAN（姿态自适应网络）检测到的面部地标进行裁剪，可以有效捕捉微表情的细微变化。下面是具体的步骤和思路：

1. **选择12个ROIs**

- 根据面部表情变化的特点，12个ROIs通常包括以下区域：
  - **左右眉毛**（2个ROIs）
  - **左右眼睛**（2个ROIs）
  - **左右脸颊**（上下各2个，共4个ROIs）
  - **鼻子**（1个ROI）
  - **嘴巴区域**（包括上下唇，共2个ROIs）
  - **下巴**（1个ROI）

2. **使用SAN检测面部地标**

- 使用SAN（姿态自适应网络）来检测面部的68个地标点。这些地标点将用于定义每个ROI的边界。例如，眼睛区域的ROI可以由对应的眉毛和眼睑的地标点确定。
- SAN的优点在于它可以在不同姿态下准确检测面部特征，确保ROI的裁剪更加精确。

3. **裁剪ROIs**

- 根据检测到的面部地标，裁剪出12个ROIs。具体方法是使用这些地标点来定义每个ROI的边界框。裁剪后，每个ROI只包含面部的一个特定区域，如眼睛或嘴巴。
- 裁剪的具体过程可以参考以下Python代码：

```python
import cv2
import numpy as np

def crop_roi(image, landmarks, points):
    # 提取ROI的边界框
    x_min = np.min(landmarks[points, 0])
    x_max = np.max(landmarks[points, 0])
    y_min = np.min(landmarks[points, 1])
    y_max = np.max(landmarks[points, 1])

    # 裁剪ROI
    roi = image[y_min:y_max, x_min:x_max]
    return roi

# 例如：定义眼睛区域的地标点索引
left_eye_points = [36, 37, 38, 39, 40, 41]  # 左眼的地标点
right_eye_points = [42, 43, 44, 45, 46, 47] # 右眼的地标点

# 使用SAN检测面部地标（假设landmarks是检测到的68个点）
# landmarks = detect_landmarks(image)

# 裁剪左眼ROI
left_eye_roi = crop_roi(image, landmarks, left_eye_points)

# 裁剪右眼ROI
right_eye_roi = crop_roi(image, landmarks, right_eye_points)

# 其他ROI可以类似裁剪
```

4. **细化特征**

- 在每个ROI内，计算光流特征或其他相关的表情特征。Liu等人提出的方法主要是基于光流的特征，即主方向平均光流（Main Directional Mean Optical Flow, MDMO）。
- 对于每个ROI，使用TV-L1算法计算光流，然后提取该区域的主方向和平均光流大小作为特征，即MDMO特征。

5. **MDMO特征计算示例**

- 计算光流之后，提取每个ROI中的光流方向，并计算这些方向的主方向及其平均光流大小。
- 将每个ROI的MDMO特征组合起来，形成最终的特征向量。这个特征向量可以作为输入，传递给分类器进行微表情识别。

```python
def calculate_mdmo(flow):
    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])
    # 计算主方向平均光流
    mean_direction = np.mean(ang)
    mean_magnitude = np.mean(mag)
    return mean_direction, mean_magnitude

# 假设我们已计算了左眼ROI的光流
left_eye_flow = optical_flow.calc(prev_left_eye, curr_left_eye, None)
left_eye_mdmo = calculate_mdmo(left_eye_flow)
```

6. **构建特征向量**

- 将每个ROI的特征组合起来，形成最终的特征向量。这个特征向量将用于后续的微表情识别模型训练和测试。

通过这种方法，能够更加精细地捕捉面部不同区域的运动模式，提升微表情识别的效果。



> 计算细粒度MDMO特征作为模型输入



#### AUW-GCN模型

##### AU先验编码模块

AU先验编码模块的逻辑：

1.详细说明如何将先验信息嵌入GCN中，并列举相关论文

2.详细说明如何表征不同兴趣区域之间的相关性，并列举相关论文

3.详细说明如何构建量化这些关系的邻接矩阵，并列举相关论文

> 如何表征不同ROIs之间的相关性以及如何构建邻接矩阵





> 将AUs的共性和相关性联系起来，并将AUs映射到人脸上的特定ROIs





> 如何使用邻接矩阵的形式量化选择的12个ROIs



> 受[19]的启发，将数据集中的AU信息作为先验信息编码到GCN中
>
> [19]Ling Lo, Hong-Xia Xie, Hong-Han Shuai, and Wen-Huang Cheng, “Mer-gcn: Micro-expression recognition based on relation modeling with graph convolutional networks,” in 2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR). IEEE, 2020, pp. 79–84.



##### 空间嵌入模块

空间嵌入模块的逻辑：

1.详细说明使用图卷积网络作为空间嵌入模块的的基本构建模块，并列出相关论文

2.详细说明空间嵌入模块以提取的MDMO特征为输入，每层经过先验编码邻接矩阵、特征嵌入矩阵、可学习权矩阵和激活函数的处理，最终输出经过图卷积后的精细化空间特征嵌入，并列出相关论文

> 使用GCN作为空间特征嵌入模块(SFEM)的基本构建块



##### 时间特征嵌入模块

时间特征嵌入模块的逻辑：

1.详细说明以空间嵌入模块的细粒度空间特征进行时间卷积运算，输出概率图，并列举相关论文



##### 生成模块

生成模块的逻辑：

1.详细说明在得到起始帧、顶点帧和偏移帧的概率序列之后，生成相应的算法

##### 后期处理

后期处理的逻辑：

1.进一步处理上一个模块提出的算法，减少高度重叠的间隔，从而提高间隔发现的质量，并列出相关论文



#### 优化

优化部分的逻辑：

1.详细说明将定位任务转化为一个分类问题，针对不同类型的帧设计了一个二分类任务和一个三分类任务，并列出相关论文

2.详细说明为了应对数据不平衡，选择Focal Loss作为基本损失函数，并列出相关论文

3.详细说明使用滑动窗口技术将视频分割成片段，并列出相关论文

> 将微表情的定位任务转化为一个常见的分类问题，针对不同类型的帧设计了一个二分类任务和一个三分类任务



> 为了应对数据不平衡，选择Focal Loss作为基本损失函数



> 使用滑动窗口技术将长视频分割成片段





### 实验

#### 实验设置

实验设置的逻辑：

1.详细说明在实验中采用了Leave-One-Subject-Out (LOSO)交叉验证策略的原因，并列举相关论文

2.详细说明CAS(ME)^2和SAMM-LV这两个数据集的信息，并列举相关论文

3.详细说明Adam优化器，学习率，apex评分阈值，NMS后处理阈值，并列举相关论文

4.详细说明真阳性TP，假阳性FP，精度，召回率，F1分数等评价指标，并列举相关论文



> 根据MEGC2021[27]的协议，我们在实验中采用了Leave-One-Subject-Out (LOSO)交叉验证策略。



#### 实验结果

实验结果的逻辑：

1.详细说明论文的方法在数据集CAS(ME)^2和SAMM-LV上的性能

2.详细说明将论文的方法、手工方法和深度学习方法进行比较

#### 案例研究

案例研究的逻辑：

1.举例说明论文方法的优势

### 总结

总结的逻辑：

1.简要说明本文的提出的方法

2.简要说明文本的创新点

3.简要说明本文方法的有效性



## 论文重点分析

1.为什么使用数据集时没有使用RGB图像？使用的是npz文件？是否可以用其他数据集替代？

2.如何改进？

3.为什么数据集的编号是从015开始，而不是从0开始？之前的数据不需要吗？



## 阅读感想

1.训练模型时，不是直接使用的图片和视频，而是使用npz数组。

2.项目中交代了面部关键点和兴趣区域的提取的具体方法

## 代码分析

1.使用两个数据集CAS(ME)^2和SAMM-LV，代码中是分别训练，但是在在最终结果统计中，综合统计了两个数据集的训练效果，~~在代码中有`cross.csv`这样类似的文件~~（在项目中发现cross是一个数据集），猜测是否使用两个数据集进行交叉训练和验证。



2.代码读入数据集时没有直接读取图片和视频，使用的是处理好的数据集，形式上是分块的`npz`文件，猜测是最初使用图片和视频以及AU信息进行处理之后形成的`npz`文件，虽然在项目中存在相关的面部信息处理的代码文件，但不能确定是否是从源图片和视频处进行处理。如果从源图片和视频进行处理，但是没从代码文件中找到相关读取图片和视频的路径。

3.为什么待处理的数据集命名不从0开始，比如casme从015开始，samm从007开始？猜测只处理视频，在项目给出的图片中的数据预处理部分，给出的是输入视频，但是还未进行验证。

4.代码的输出结果没有输出图片一类，可能需要自己写一部分代码，将输出的结果使用曲线图或折线图进行表示。

5.输出结果中还有`pth`一类的文件，但是是分块的，一个数据集有多个`pth`文件。一般认为跑一次代码后获得的模型，下一次跑代码时可以使用上一次的模型，但是在代码中没找到类似导入之前的模型的代码，不过找到数据预处理的一个函数，这个函数中的读入的路径是项目中不存在的路径。所以是否需要自己写相关的读入上次训练的模型的代码？

6.项目中提供的csv文件与数据集中的xlsx文件中的au区域有冲突，xlsx文件中有的au值为null，不知xlsx中的数据是否值得可信？暂时先用项目中的csv文件。

7.项目中有原始视频和图像处理的代码，第一次运行时出现了各种意外的错误。





